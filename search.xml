<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[memcathed+tomcat7实现session共享搭建环境与问题解决]]></title>
      <url>%2F2017%2F04%2F06%2Fmemcathed%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%E5%8F%8A%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
      <content type="text"><![CDATA[memcathed 下载安装1. 下载安装包，下载地址1.4.362. linux下安装 memcached2.1 安装libevent-2.0.15 安装之前，需要先确认系统中是否有libevent，因为memcached依赖这个包。查看:命令： rpm -qa|grep libevent显示的包：libevent-1.4.13-4.el6.x86_64此时，系统已经安装过了，需要卸载重新下载安装。卸载：rpm -e libevent-1.4.13-4.el6.x86_64 下载libevent，并安装 下载地址 解压：tar -zxvf libevent-2.0.21-stable.tar.gz安装：./configure –prefix=/usr 编译：makemake install安装完成！2.2 下载memcached,并解压命令： tar -zxvf memcached-1.4.36.tar.gz 安装：./configure –with-libevent=/usr若安装过程中提示找不到libevent路径时，使用–with-libevent=libevent安装的目录./configure –prefix=/usr –with-libevent=/usr编译：makemake install安装完成！ 启动/usr/local/bin/memcached -d -m 10 -u root -l 192.168.158.135 -p 11211 -c 256 -P /usr/memcached.pid启动参数说明：-d 选项是启动一个守护进程。-u root 表示启动memcached的用户为root。-m 是分配给Memcache使用的内存数量，单位是MB，默认64MB。-M return error on memory exhausted (rather than removing items)。-u 是运行Memcache的用户，如果当前为root 的话，需要使用此参数指定用户。-p 是设置Memcache的TCP监听的端口，最好是1024以上的端口。-c 选项是最大运行的并发连接数，默认是1024。-P 是设置保存Memcache的pid文件。 停止kill cat /usr/memcached.pid 或者先查看进程的idps -ef|grep memcachedroot 15144 1 0 08:43 ? 00:00:00/usr/local/bin/memcached -d -m 10 -u root -l 192.168.158.135 -p 11211 -c 256 -P /usr/memcached.pid15144为pid停止命令为：kill -9 15144 3 配置tomcat7 msm的配置可以参考google官方说明，但官方推荐的JAR包版本过老且缺少依赖如果使用TOMCAT7.0.60以上的版本会出现问题。如出现: tomcat启动正常，访问时出现异常，信息：http11.AbstractHttp11Processor process java.lang.NoSuchFieldError: attributes处理：修改为粘性（sticky）方式可以访问（这里官方文档还有问题在只有1个节点时，不能指定failoverNodes=”n1” ），但因为使用nginx，并且nginx非最外层，客户的ip是动态的等原因不能是ip_hash进行处理，存在多台tomcat，必须使用非粘性（non-sticky）经过查找终于发现msm1.9.3之后fix了这个问题，所以进行版本升级：添加memcached和msm(Memcached_Session_Manager)的依赖jar包，如下12345678910spymemcached-2.12.0.jarreflectasm-1.10.1.jarobjenesis-2.1.jarmsm-kryo-serializer-1.9.5.jarminlog-1.3.0.jarmemcached-session-manager-tc7-1.9.5.jarmemcached-session-manager-1.9.5.jarkryo-serializers-0.37.jarkryo-3.0.3.jarasm-5.0.3.jar 修改tomcat7下的 conf/context.xml12345678&lt;Manager className="de.javakaffee.web.msm.MemcachedBackupSessionManager"memcachedNodes="n1:wwww.demo.com:11211,n2:www.demo2.com:11211"sticky="false"sessionBackupAsync="false"sessionBackupTimeout="1000"lockingMode="uriPattern:/path1|/path2"requestUriIgnorePattern=".*\.(ico|png|gif|jpg|css|js)$"transcoderFactoryClass="de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory" /&gt; 测试是否实现session共享对2个tomcat进行相同配置后可分别向2个tomcat中增加项目AA中存放一个用来测试的DEMO.JSP内容如下12345 SessionID:&lt;%=session.getId()%&gt;&lt;br&gt;SessionIP:&lt;%=request.getServerName()%&gt;&lt;br&gt;SessionPort:&lt;%=request.getServerPort()%&gt; 刷新2个tomcat 项目下的demo页面发现SESSION 相同则配置成功。 注意问题如果在一台机器上同时启动2个tomcat，一个配置了memcathed一个使用本地内存来储存SESSION,2个项目同时启动并运行同一个项目时，会导致memcathed+tomcat 的环境错误导致memcathed断开，SESSION不断变化。但这种情况与正常的memcathed服务器掉线导致的断开session变化不同。因为没有配置memcathed的tomcat启动后也向页面写入了cookie这样会导致使用memcathed管理session的tomcat项目分配的cookie会始终与memcathed服务端记录的cookie不同导致项目无法再次重新连接到memcathed。解决办法，清除浏览器cookie。为避免上述问题出现，可以对tomcat进行配置时每个tomcat一台机器上的tomcat单独使用一块内存。对每一个tomcat设置独立的区域]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用supermap把矢量地图转换为svg地图]]></title>
      <url>%2F2017%2F03%2F03%2F%E4%BD%BF%E7%94%A8supermap%E6%8A%8A%E7%9F%A2%E9%87%8F%E5%9C%B0%E5%9B%BE%E8%BD%AC%E6%8D%A2%E4%B8%BAsvg%E5%9C%B0%E5%9B%BE%2F</url>
      <content type="text"><![CDATA[supermap to svg 最近看到了一个把矢量地图制作成SVG地图的功能，个人觉得挺花哨的研究了一下，把研究经验分享一下，如果有更好的方法请跟我联系大家可以探讨 使用supermap idesktop 把一个线对象导出为 Autocad dxf 文件 使用工具DWG to SVG Converter MX 把 dxf 转换为 svg 文件 使用工具Inkscape 给闭环的线填充内容获取填充后的 svg 图形 使用 Raphael js 绘制svg添加效果]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[javascript小数运算]]></title>
      <url>%2F2017%2F02%2F22%2Fjavascript%E5%B0%8F%E6%95%B0%E8%BF%90%E7%AE%97%2F</url>
      <content type="text"><![CDATA[javascript 小数运算 12console.log(0.1 + 0.2); //0.30000000000000004console.log(0.1 + 0.2 == 0.3); //false JavaScript 中的 number 类型就是浮点型，JavaScript 中的浮点数采用IEEE-754 格式的规定，这是一种二进制表示法，可以精确地表示分数，比如1/2，1/8，1/1024，每个浮点数占64位。但是，二进制浮点数表示法并不能精确的表示类似0.1这样 的简单的数字，会有舍入误差。 由于采用二进制，JavaScript 也不能有限表示 1/10、1/2 等这样的分数。在二进制中，1/10(0.1)被表示为 0.00110011001100110011…… 注意 0011 是无限重复的，这是舍入误差造成的，所以对于 0.1 + 0.2 这样的运算，操作数会先被转成二进制，然后再计算： 0.1 =&gt; 0.0001 1001 1001 1001…（无限循环） 0.2 =&gt; 0.0011 0011 0011 0011…（无限循环） 双精度浮点数的小数部分最多支持 52 位，所以两者相加之后得到这么一串 0.0100110011001100110011001100110011001100…因浮点数小数位的限制而截断的二进制数字，这时候，再把它转换为十进制，就成了 0.30000000000000004。 对于保证浮点数计算的正确性，有两种常见方式。 一是先升幂再降幂： 12345678910function add(num1, num2)&#123; let r1, r2, m; r1 = (''+num1).split('.')[1].length; r2 = (''+num2).split('.')[1].length; m = Math.pow(10,Math.max(r1,r2)); return (num1 * m + num2 * m) / m;&#125;console.log(add(0.1,0.2)); //0.3console.log(add(0.15,0.2256)); //0.3756 二是是使用内置的 toPrecision() 和 toFixed() 方法，注意，方法的返回值字符串。 1234function add(x, y) &#123; return x.toPrecision() + y.toPrecision()&#125;console.log(add(0.1,0.2)); //"0.10.2"]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hive安装配置使用]]></title>
      <url>%2F2017%2F02%2F08%2Fhive%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[HIVE简介 HIVE下载，安装下载下载地址 安装安装MYSQL1234#CentOSyum install mysql mysql-connector-java#启动Mysqlservice mysqld start 安装HIVE1tar -zxvf apache-hive-2.1.0-bin.tar.gz -C usr/hadoop HIVE配置配置文件hive-env.sh|hive-stie.xml hive-env.sh 1HADOOP_HOME=/etc/hadoop/hadoop-2.6.0 hive-stie.xmlss 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file. --&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;&lt;!--metastore的端口--&gt;&lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://hadoop:9083&lt;/value&gt;&lt;/property&gt;&lt;!--HiveServer2的端口--&gt;&lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;beeline.hs2.connection.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;beeline.hs2.connection.password&lt;/name&gt; &lt;value&gt;111111&lt;/value&gt; &lt;/property&gt; &lt;!--使用mysql存储元数据--&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://127.0.0.1/metastore?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;sa&lt;/value&gt; &lt;/property&gt; &lt;!--hive在HDFS上的存储路径--&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop:9000&lt;/value&gt; &lt;/property&gt; &lt;!--此外我们还配置自动创建Meta Store的数据库和表--&gt; &lt;property&gt; &lt;name&gt;datanucleus.autoCreateSchema&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;datanucleus.autoStartMechanism&lt;/name&gt; &lt;value&gt;SchemaTable&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;datanucleus.schema.autoCreateTables&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; HIVE使用启动 启动hive组件 1234#启动MetaStore Servernohup hive --service metastore &gt;&gt; /usr/hadoop/apache-hive-2.1.0-bin/logs/metastore.log 2&gt;&amp;1 &amp;#启动HiveServer2nohup hive --service hiveserver2 &gt;&gt; /usr/hadoop/apache-hive-2.1.0-bin/logs/hive.log 2&gt;&amp;1 &amp; hive启动hive beeline启动12beelinebeeline&gt; !connect jdbc:hive2://localhost:10000/default root 111111 建表123456789create table if not exists user_dimension (uid STRING,name STRING,gender STRING,birth DATE,province STRING)ROW FORMAT DELIMITEDFIELDS TERMINATED BY ','&lt;!--创建完后 可以使用 shwo create tabel &lt;tablename&gt; 查看详细的建表过程。--&gt; 导入数据1load data inpath '/flume/record/2017-01-06/0500' overwrite into table &lt;tablename&gt;; 执行SQL脚本1hive -f /usr/downgit/hadooptraining/hive/command/create_table_record.sql 导出数据到HDFS123exportinsert overwrite directory '/demo'select sum(price),source_province from record group by source_province;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[flume安装配置使用]]></title>
      <url>%2F2017%2F02%2F08%2Fflume%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[FLUME介绍flume是一个分布式的，可靠的，可用的服务，有效地收集，汇总和移动大量的日志数据。它有一个简单的和灵活的架构基于流数据流。具有很好的健壮性和容错性具有可调性可靠性机制和多故障转移和恢复机制。它使用一个简单的可扩展数据模型，允许联机分析应用程序。 flume下载flume直接在官网提供最新版本下载，同时也提供了历史版本下载 flume安装 下载flume后直接解压到文件夹 1$ tar -zxvf apache-flume-1.7.0-bin.tar.gz -C usr/hadoop/ 修改配置文件 1flume-env.ps1.template flume-env.ps1.sh 修改文件内容 12export JAVA_HOME=/usr/java/jdk1.8.0_111FLUME_CLASSPATH="$HADOOP_HOME/share/hadoop/common/hadoop-common-2.6.0.jar" 修改profile文件增加flume 12export FLUME_HOME=/etc/hadoop/apache-flume-1.7.0-binexport PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$FLUME_HOME/bin fulme使用 监听一个文件夹有新文件进入后导入到HDFS中 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# Licensed to the Apache Software Foundation (ASF) under one# or more contributor license agreements. See the NOTICE file# distributed with this work for additional information# regarding copyright ownership. The ASF licenses this file# to you under the Apache License, Version 2.0 (the# "License"); you may not use this file except in compliance# with the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing,# software distributed under the License is distributed on an# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY# KIND, either express or implied. See the License for the# specific language governing permissions and limitations# under the License.# The configuration file needs to define the sources,# the channels and the sinks.# Sources, channels and sinks are defined per agent,# in this case called 'agent'logAgent.sources = logSourcelogAgent.channels = fileChannellogAgent.sinks = hdfsSink# For each one of the sources, the type is definedlogAgent.sources.logSource.type = spooldir#监听的文件夹logAgent.sources.logSource.spoolDir = /root/bigdata/datasourcelogAgent.sources.logSource.basenameHeader = truelogAgent.sources.logSource.basenameHeaderKey = basename# The channel can be defined as follows.logAgent.sources.logSource.channels = fileChannel# Each sink's type must be definedlogAgent.sinks.hdfsSink.type = hdfs#导入HDFS的位置logAgent.sinks.hdfsSink.hdfs.path = hdfs://hadoop:9000/flume/weatherlogAgent.sinks.hdfsSink.hdfs.filePrefix= %&#123;basename&#125;logAgent.sinks.hdfsSink.hdfs.rollInterval= 600logAgent.sinks.hdfsSink.hdfs.rollCount= 10000logAgent.sinks.hdfsSink.hdfs.rollSize= 0logAgent.sinks.hdfsSink.hdfs.round = truelogAgent.sinks.hdfsSink.hdfs.roundValue = 1logAgent.sinks.hdfsSink.hdfs.roundUnit = minutelogAgent.sinks.hdfsSink.hdfs.fileType = DataStreamlogAgent.sinks.hdfsSink.hdfs.useLocalTimeStamp = true#Specify the channel the sink should uselogAgent.sinks.hdfsSink.channel = fileChannel# Each channel's type is defined.logAgent.channels.fileChannel.type = filelogAgent.channels.fileChannel.checkpointDir= /etc/hadoop/apache-flume-1.7.0-bin/dataCheckpointDirlogAgent.channels.fileChannel.dataDirs= /etc/hadoop/apache-flume-1.7.0-bin/dataDir# Other config values specific to each type of channel(sink or source)# can be defined as well# In this case, it specifies the capacity of the memory channel 启动脚本 1flume-ng agent --conf /etc/hadoop/apache-flume-1.7.0-bin/conf/ --conf-file /etc/hadoop/apache-flume-1.7.0-bin/conf/flume-conf-weather.properties --name logAgent -Dflume.root.logger=DEBUG,console -Dflume.monitoring.type=http -Dflume.monitoring.port=34545]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[sqoop安装配置使用]]></title>
      <url>%2F2017%2F02%2F07%2Fsqoop%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[sqoop介绍Sqoop(发音：skup)是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。 sqoop下载，安装下载下载地址 安装下载好后解压文件1$ tar -zxvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C usr/sqoop sqoop配置修改 $sqoop_home/conf 下的模板文件1mv sqoop-env-template.cmd sqoop-env.sh 增加export HADOOP_COMMON_HOME=/etc/hadoop/hadoop-2.6.0 sqoop导入导出语句从数据库导入到HDFS1sqoop import --connect jdbc:mysql://117.37.36.199:3306/log --username root --password root --table user_dimension --driver com.mysql.jdbc.Driver --m 10 --target-dir /warehouse/user_dimension 参数 说明 –append 将数据追加到hdfs中已经存在的dataset中。使用该参数，sqoop将把数据先导入到一个临时目录中，然后重新给文件命名到一个正式的目录中，以避免和该目录中已存在的文件重名。 –as-avrodatafile 将数据导入到一个Avro数据文件中 –as-sequencefile 将数据导入到一个sequence文件中 –as-textfile 将数据导入到一个普通文本文件中，生成该文本文件后，可以在hive中通过sql语句查询出结果。 –boundary-query 边界查询，也就是在导入前先通过SQL查询得到一个结果集，然后导入的数据就是该结果集内的数据，格式如：–boundary-query ‘select id,creationdate from person where id = 3’，表示导入的数据为id=3的记录，或者select min(), max() from ，注意查询的字段中不能有数据类型为字符串的字段，否则会报错：java.sql.SQLException: Invalid value for getLong() 目前问题原因还未知 –columns 指定要导入的字段值，格式如：–columns id,username –direct 直接导入模式，使用的是关系数据库自带的导入导出工具。官网上是说这样导入会更快 –direct-split-size 在使用上面direct直接导入的基础上，对导入的流按字节数分块，特别是使用直连模式从PostgreSQL导入数据的时候，可以将一个到达设定大小的文件分为几个独立的文件。 –inline-lob-limit 设定大对象数据类型的最大值 -m,–num-mappers 启动N个map来并行导入数据，默认是4个，最好不要将数字设置为高于集群的节点数 –query，-e 从查询结果中导入数据，该参数使用时必须指定–target-dir、–hive-table，在查询语句中一定要有where条件且在where条件中需要包含$CONDITIONS，示例：–query ‘select * from person where $CONDITIONS ‘ –target-dir /user/hive/warehouse/person –hive-table person –split-by |表的列名，用来切分工作单元，一般后面跟主键ID–table |关系数据库表名，数据从该表中获取–target-dir |指定hdfs路径–warehouse-dir |与–target-dir不能同时使用，指定数据导入的存放目录，适用于hdfs导入，不适合导入hive目录–where 从关系数据库导入数据时的查询条件，示例：–where ‘id = 2’-z,–compress |压缩参数，默认情况下数据是没被压缩的，通过该参数可以使用gzip压缩算法对数据进行压缩，适用于SequenceFile, text文本文件, 和Avro文件–compression-codec |Hadoop压缩编码，默认是gzip–null-string |可选参数，如果没有指定，则字符串null将被使用–null-non-string |可选参数，如果没有指定，则字符串null将被使用 从HDFS导入数据库1sqoop export --table WFC_WEATHER -connect "jdbc:mysql://117.37.36.199:3306/hadoopcg?useUnicode=true&amp;characterEncoding=UTF-8" --username root --password root --export-dir /demos/part-r-00000 --input-fields-terminated-by ',' -m 10 参数 说明 –direct 快速模式，利用了数据库的导入工具，如mysql的mysqlimport，可以比jdbc连接的方式更为高效的将数据导入到关系数据库中。 –export-dir 存放数据的HDFS的源目录 -m,–num-mappers 启动N个map来并行导入数据，默认是4个，最好不要将数字设置为高于集群的最大Map数 –table 要导入到的关系数据库表 –update-key 后面接条件列名，通过该参数，可以将关系数据库中已经存在的数据进行更新操作，类似于关系数据库中的update操作 –update-mode 更新模式，有两个值updateonly和默认的allowinsert，该参数只能是在关系数据表里不存在要导入的记录时才能使用，比如要导入的hdfs中有一条id=1的记录，如果在表里已经有一条记录id=2，那么更新会失败。 –input-null-string 可选参数，如果没有指定，则字符串null将被使用 –input-null-non-string 可选参数，如果没有指定，则字符串null将被使用 –staging-table 该参数是用来保证在数据导入关系数据库表的过程中事务安全性的，因为在导入的过程中可能会有多个事务，那么一个事务失败会影响到其它事务，比如导入的数据会出现错误或出现重复的记录等等情况，那么通过该参数可以避免这种情况。创建一个与导入目标表同样的数据结构，保留该表为空在运行数据导入前，所有事务会将结果先存放在该表中，然后最后由该表通过一次事务将结果写入到目标表中。 –clear-staging-table 如果该staging-table非空，则通过该参数可以在运行导入前清除staging-table里的数据。 –batch 该模式用于执行基本语句（暂时还不太清楚含义）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hadoop环境搭建-hadoop2.6.0安装配置，及环境概述]]></title>
      <url>%2F2017%2F02%2F06%2Fhadoop2.6.0%E5%AE%89%E8%A3%85%2F</url>
      <content type="text"><![CDATA[前一阵子弄了1个多月的hadoop环境，基本把环境安装了并且能够进行简单的数据分析，把安装的过程记录下来供参考。 前言因为在windows上进行的研究，全程使用VMware虚拟机，会专门写篇文章将VMware虚拟这里不详细介绍了。 环境准备安装的CentOS6.5系统，用到的软件有名称|版本|地址–|–|–hadoop|2.6.0|http://apache.fayea.com/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gzsqoop|1.4.6|http://mirror.bit.edu.cn/apache/sqoop/1.4.6/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gzflume|1.7.0|http://mirrors.hust.edu.cn/apache/flume/1.7.0/apache-flume-1.7.0-bin.tar.gzhive|2.1.0|http://mirror.bit.edu.cn/apache/hive/stable-2/apache-hive-2.1.0-bin.tar.gzpresto|0.157|https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.157/presto-server-0.157.tar.gzpresto-cli|0.157|https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.157/presto-cli-0.157-executable.jar 环境搭建 ps:推荐最高就用2.6.0的环境了，因为官方给的windows调试版本就是2.6.0，在高的版本没有 winutil.exe 与 hadoop.dll 这2个文件在后续的开发中，无法再windows端进行调试。 伪分布式环境搭建基础配置 hadoop使用JAVA开发，所以需要的最基本环境就是JAVA，安装JDK 创建文件夹 1mkdir /usr/java 解压 1tar -zxvf jdk-1.8.0_111-linux.tar.gz -C /usr/java/ 将java添加到环境变量中 1vim /etc/profile 123#在文件最后添加export JAVA_HOME=/usr/java/jdk1.8.0_111export PATH=$PATH:$JAVA_HOME/bin 刷新配置 1source /etc/profile 下载hadoop 將下载好的hadoop2.6.0上传到/usr/hadoop 解压 1tar -zxvf hadoop-2.6.0.tar.gz 修改 /usr/hadoop/hadoop2.6.0/etc/hadoop下的5个配置文件，分别为core-site.xml|hdfs-site.xml|hadoop-env.sh|mapred-site.xml|yarn-site.xml core-site.xml 增加内容 12345678910111213141516171819 &lt;!-- 制定HDFS（NameNode）的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/hadoop-2.6.0/tmp&lt;/value&gt; &lt;/property&gt;&lt;!-- 配置当前用户，我这里是root为所有用户和组，表示可以使用任意用户向HDFS中增加数据，调试时使用，方便调试--&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; hdfs-site.xml 增加内容 123456789&lt;!-- 指定HDFS副本的数量，伪分布指定为1 --&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;!-- 关闭HDFS的权限控制，使任意用户可以操作，开发时使用，方便本地调试MAPREDUCE--&gt;&lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; hadoop-env.sh 修改内容 1234#JDK安装路径export JAVA_HOME=/usr/java/jdk1.8.0_111export HADOOP_CONF_DIR=$&#123;HADOOP_CONF_DIR:-"/etc/hadoop"&#125; mapred-site.xml 增加内容 12345&lt;!-- 指定mr运行在yarn上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; yarn-site.xml 增加内容 12345678910&lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop&lt;/value&gt;&lt;/property&gt;&lt;!-- reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt; 修改 /etc/profile 配置HADOOP_HOME 12export HADOOP_HOME=/etc/hadoop/hadoop-2.6.0export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 刷新配置 1source /etc/profile 启动命令 第一次启动需要格式化NameNode，以后再次启动时不需要格式化。 1hadoop namenode -format 执行脚本启动hdfs 1start-dfs.sh 执行脚本启动yarn 1start-yarn.sh 验证启动使用jps命令验证12345627408 NameNode28218 Jps27643 SecondaryNameNode28066 NodeManager27803 ResourceManager27512 DataNode 访问地址http://192.168.8.118:50070 （HDFS管理界面）http://192.168.8.118:8088 （MR管理界面） IP地址为机器地址。 如果不能访问可能是因为防火墙未关闭使用命令关闭防火墙12345678#查看防火墙状态service iptables status#关闭防火墙service iptables stop#查看防火墙开机启动状态chkconfig iptables --list#关闭防火墙开机启动chkconfig iptables off 配置SSH免密码登录进入到我的home目录123456cd ~/.sshssh-keygen -t rsa （四个回车）执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）将公钥拷贝到要免登陆的机器上ssh-copy-id localhostscp -r authorized_keys root@192.168.109.130:/root/.ssh/ 之后每次启动都不需输入密码了]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ShadowSocks（影梭）]]></title>
      <url>%2F2017%2F02%2F06%2FShadowSocks%EF%BC%88%E5%BD%B1%E6%A2%AD%EF%BC%89%2F</url>
      <content type="text"><![CDATA[影梭也用了一段时间，最近刚好把使用中遇到的问题分享一下。里面好些内容都是网上摘抄的，感觉比我自己讲的好很多，看了很受启发也就拿来用了。 什么是 ShadowSocks （影梭）ShadowSocks 是由@clowwindy所开发的一个开源 Socks5 代理。如其官网所言 ，它是 “A secure socks5 proxy, designed to protect your Internet traffic” （一个安全的 Socks5 代理）。其作用，亦如该项目主页的 wiki（中文版） 中所说，“A fast tunnel proxy that helps you bypass firewalls” （一个可穿透防火墙的快速代理）。 不过，在中国，由于GFW的存在，更多的网友用它来进行科学上网。 ShadowSocks 的原理这里推荐“写给非专业人士看的 Shadowsocks 简介”，讲的非常清楚。为防止有同学无法访问该文章，这里摘抄出来： long long ago… 在很久很久以前，我们访问各种网站都是简单而直接的，用户的请求通过互联网发送到服务提供方，服务提供方直接将信息反馈给用户 when evil comes 然后有一天，GFW 就出现了，他像一个收过路费的强盗一样夹在了在用户和服务之间，每当用户需要获取信息，都经过了 GFW，GFW将它不喜欢的内容统统过滤掉，于是客户当触发 GFW 的过滤规则的时候，就会收到 Connection Reset 这样的响应内容，而无法接收到正常的内容 ssh tunnel 聪明的人们想到了利用境外服务器代理的方法来绕过 GFW 的过滤，其中包含了各种HTTP代理服务、Socks服务、VPN服务… 其中以 ssh tunnel 的方法比较有代表性 1) 首先用户和境外服务器基于 ssh 建立起一条加密的通道2-3) 用户通过建立起的隧道进行代理，通过 ssh server 向真实的服务发起请求4-5) 服务通过 ssh server，再通过创建好的隧道返回给用户 由于 ssh 本身就是基于 RSA 加密技术，所以 GFW 无法从数据传输的过程中的加密数据内容进行关键词分析，避免了被重置链接的问题，但由于创建隧道和数据传输的过程中，ssh 本身的特征是明显的，所以 GFW 一度通过分析连接的特征进行干扰，导致 ssh存在被定向进行干扰的问题 shadowsocks 于是 clowwindy 同学分享并开源了他的解决方案 简单理解的话，shadowsocks 是将原来 ssh 创建的 Socks5 协议拆开成 server 端和 client 端，所以下面这个原理图基本上和利用 ssh tunnel 大致类似 1、6) 客户端发出的请求基于 Socks5 协议跟 ss-local 端进行通讯，由于这个 ss-local 一般是本机或路由器或局域网的其他机器，不经过 GFW，所以解决了上面被 GFW 通过特征分析进行干扰的问题2、5) ss-local 和 ss-server 两端通过多种可选的加密方法进行通讯，经过 GFW 的时候是常规的TCP包，没有明显的特征码而且 GFW 也无法对通讯数据进行解密3、4) ss-server 将收到的加密数据进行解密，还原原来的请求，再发送到用户需要访问的服务，获取响应原路返回 Shadowsocks 的优劣优势 支持远程 DNS 解析，防止 DNS 污染。由于 socks5 代理支持远程 DNS 解析，因此不用另外再去找国外的 DNS 服务器，DNS查询直接递给远程代理服务器，然后通过墙外 DNS 服务器查询得到结果再传回客户端。从而 DNS 污染鞭长莫及。 安全。所有数据流量全部经过加密，加密算法可选并支持自定义算法。另外，远程 DNS 解析也使得本地的 ISP 无法通过 DNS 查询获取你所访问的网站。 隐蔽。 OPENVPN 和 VPNgate 都是死在了特征检测上，通常来说基于证书的身份认证过程和密钥交换过程都会带来独特的协议指纹（ OPENVPN 有着一套复杂完善的身份认证机制，估计 GFW 就是识别出了这一机制的协议指纹从而成功干掉 OPENVPN 的），从而使得他们在 handshake 阶段就被 GFW 识别出来并阻断了；但 ShadowSocks 直接放弃了服务器端身份认证，也抛弃了密钥协商过程（ TLS 连接就是在 handshake 阶段协商出随机密钥的），而是采取事先在服务器端设置好固定密钥的方式来应对加密连接的（设置shadowsocks客户端和服务器端的时候要填写同一个密码，这就是事先设置好的用于加密和解密的密钥）。这样做就大大减少了协议特征，再加上一般的 ShadowSocks 服务器端都是个人租用的专用服务器，流量很小，从而很难被 GFW 发现和封杀。 速度相对较快。由于其隐蔽性，只会有很少的数据包会被 GFW 丢弃，从而保证了连接速度。 连接稳定。同样由于其隐蔽性，较小的丢包率带来的是稳定的连接。智能切换。 传统的 VPN 方式，在切换网络时非常不方便， 比如连上国外的 VPN 之后会发现访问国内的网站速度严重下降。 ShadowSocks 支持 PAC 列表，根据 PAC 中的规则，有针对性地选用恰当的网络访问方式，兼顾了访问速度与访问效率。移动客户端还支持针对不同应用设置单独代理。 去中心化。服务器端搭建方便快捷，每个人都可以自己动手搭建属于自己的服务器端。部分人以免费或者收费方式共享自己的服务器，即使不想动手搭建的也有很多的免费账号或购买渠道可以选择。代码开源。不像某些蜜罐式的翻墙工具，开源的代码保证了无后门，从而为上网的隐私性与安全性提供保障。客户端配置简单。配置时只需要填写 IP /域名、端口号，密码，然后选择加密方式即可。客户端绿色小巧。Windows版本的客户端只有200多k，而且免安装，解压即可使用。 省电。在移动端上使用，电量管理中几乎看不到它的身影。支持开机启动，断网无影响，无需手动重连，方便网络不稳定或者3G&amp;Wi-Fi频繁切换的小伙伴。跨平台。支持主流系统包括 Windows， Linux， Mac， Android, IOS，都有对应的客户端支持。 劣势若自己搭建 ShadowSocks 服务，需要一定的 技术成本。因为大部分服务端是基于运行 Linux 的 VPS（虚拟主机）搭建，因此需要学会使用 putty 等远程管理工具的使用方法，并掌握一定的Linux基本命令行操作。金钱成本。租用国外的服务器，需要价格不菲的费用。若使用免费 ShadowSocks 账号，可能有一定的风险。 时间精力成本。很多免费账号由于使用者众多，人均流量很小，导致网速慢，不稳定，需要不断寻找新的替代，且很多免费账号会定时更新密码，或者是有流量限制，或者是需要定时签到等等，这些都需要花费时间和精力去一一满足需求才能短暂使用。安全成本。有一些人会将自己的 ShadowSocks 节点免费分享出来，这些人是非常值得肯定和称赞的！但是其中有一小部分人别有居心，例如在 ShadowSocks 的服务器端监听网络流量，进行中间人攻击等，这类钓鱼服务器会严重损害用户的个人信息安全。若购买收费的 ShadowSocks 账号， 需要一定的金钱。 金钱成本 由于很多 ShadowSocks 卖家都是以盈利为目的，需要花费一定的费用。那么，自己想要有一个影梭账号的话，到底是选用哪种比较好呢？可以看到网上很多卖 ShadowSocks 账号的，有的价格很便宜，比自己搭服务器划算多了，是不是直接购买一个账号比较好呢？在这里我建议大家自己搭服务器，或者找几个人一起租个服务器比较划算。不推荐购买商业出售的 ShadowSocks。商业的出售 ShadowSocks 账号的行为绝大部分都是耍流氓。试着算一下，作为一个 ShadowSocks 出售者，他们的成本是服务器的租用费用，而收入是购买 ShadowSocks 账号人数 × 每人花费的 ShadowSocks 账号购买价格。我们假设，一台服务器的租用成本，是 10 美元一个月，那么如果要想不亏本，且做到低价，假设现在卖给十个人，那么至少每个人每月要 1 美元。但是，服务器的带宽是有限的，假设是 100M 的带宽，那么平均分下来人均带宽只有 10M，而服务器的流量一般也是有限的，如果一共是 1000 M 的流量，那么每人每月只分到 100M 。如果超出流量，服务器的租用费用会增加，就好像我们手机流量超出后，额外的流量需要交钱一样。因此，购买的流量一般都会受限制。从上面的例子可以看到，服务器资源是大家共享的，使用的人越多，人均分到的资源就越少。但是， ShadowSocks 的卖家需要赚钱，那么怎么办呢？当然是最小化成本，并最大化收入了。也就是说，尽量租用少量廉价的服务器，然后将它以尽可能高的价格卖给尽可能多的消费者。因此，对于 ShadowSocks 的商家而言，超售（即一台服务器原本最多10个人用的，可能最后卖了100个人）的现象非常严重，消费者最终所能享受到的流量和连接速度显然对不起自己的花费。不如自己租个服务器，不仅能有一个稳定放心的服务器，还能顺便学习些 Linux 的相关知识。如果嫌麻烦，可以找那种大家一起租一个服务器的，相对而言，至少有一点可以保证，就是不会有各种奇奇怪怪的限制，比如不能发邮件，不能看视频，不能下载特定的某些资源等等。而且最大的一个好处是，租用的人数一般较少，最重要的是这种一般不用担心超售，各方面性能配置等有保证。ShadowSocks 配置 服务器端配置服务器的选择首先是选择一款合适的国外主机作为服务器，一般而言，用来作 ShadowSocks 的服务器的话，并不需要一台独立的国外主机，只需要选择一款虚拟主机（Virtual Private Server，简称 VPS）即可满足需要。这里推荐几家较常用的VPS。 BandwagonHOST（俗称搬瓦工）,Vultr,Linode,枫叶说实话除了搬瓦工都不推荐，其他的太贵了，而且搬瓦工有一键部署SS服务端的功能，也支持支付宝支付，对于只作为翻墙用的服务器是首选。 服务端安装安装 python 的 pip1yum install python-setuptools &amp;&amp; easy_install pip 然后安装 shadowsocks1pip install shadowsocks 增加配置文件1vi /etc/shadowsocks.json 内容如下12345678910&#123; "server":"your_server_ip", "server_port":8388, "local_address": "127.0.0.1", "local_port":1080, "password":"auooo.com", "timeout":300, "method":"aes-256-cfb", "fast_open": false&#125; 名称 说明 server 填入你的服务器 IP ，即当前操作的 VPS 的 IP 地址，必须修改 server_port 服务器端口，可以根据实际需要修改，或者保持默认 local_address 本地监听地址，建议保持默认 local_port 本地端口，这个参数一般保持默认即可 password 用来加密的密码，可以根据实际需要修改 timeout 单位秒，一般保持默认即可 method 默认的是”aes-256-cfb”，一般保持默认即可 fast_open 使用TCP_FASTOPEN, 参数选项true / false，一般保持默认即可 workers worker的数量, 在 Unix/Linux 上有效，一般不用加此项 启动1ssserver -c /etc/shadowsocks.json -d start 停止1ssserver -c /etc/shadowsocks.json -d stop 增加开机启动1vi /etc/rc.local 增加1ssserver -c /etc/shadowsocks.json -d start 搬瓦工一键安装搬瓦工开发了一键安装SS服务的功能，在控制台中选择安装就OK。[server 安装](http://shadowsocks.org/en/download/servers.html)[配置文件](http://shadowsocks.org/en/config/quick-guide.html) 客户端客户端下载下载地址提取码:z6d6 客户端配置配置在服务端JSON中设置的值就可以了 作者回答问题 当时几个版本内置的公共服务器流量爆了，打算把公共服务器给 @showfom 运营，并帮 @showfom 架设了服务。但没想到实际情况变成了用名气来进行宣传，也没想到这个服务会被用户当成“官方”，所以为了避免这种误解，加上精力也不够，还是取消了合作，没有收一分钱。在 Twitter 发过澄清，没在 V2EX 发是一个疏忽。至于在代码里加红字警告? Are you kidding? 一个开源项目，代码本身应该是中立的，是不应该有立场的。 为了避免今后出现类似的情况，在这里做一个声明：Shadowsocks 是一个由很多人参与的开源项目，它只是一个程序，一套算法。这些人是以自由的形式凭自己的兴趣参与这个项目的，不存在“Shadowsocks 团队”，只存在贡献者。哪天现在的这些人不维护这些代码了，也会有其他人继续维护下去。至于 Shadowsocks 的使用，只要不违反开源授权，不会干涉。 公共服务器还是免费，只是做了限速以免流量用超。OS X 版的公共服务器速度慢到不能用的时候直接关掉了。和 GFW 捉迷藏换 IP 的游戏，虽然已经半自动化，但一点也不好玩，你们明白 fqrouter 为什么要关了吧。 从实际情况上来看，Shadowsocks 没有办法离开去中心化的服务器。要么自己花钱买 VPS，要么用有人分享的账号，要么用有人提供的付费服务，他们各有所长，适合不同的人。所以作为开发者，保持中立，不偏袒其中任何一方，顺其自然发展下去是最好的吧。 很多人要么一窝蜂的支持，要么一窝蜂的反对，还要把它给封禁掉，大概这种心理鲁迅先生也曾批判过。我还记得当年极路由在 V2EX 被冤枉的时候，也是一边倒的说极路由窃取隐私。如果你们真的那么讨厌商业，那你们应该首先把你们的苹果设备给摔了，因为它就是商业社会巅峰造极的产物。我反对不喜欢一个东西就要拿出简单粗暴的制裁手段，正是这种习性成就了 GFW。 维护这个项目到现在大概总共回复过几千个问题，开始慢慢想清楚了一件事，为什么会存在 GFW。从这些提问可以看出，大部分人的自理能力都很差，只是等着别人帮他。特别是那些从 App Store 下载了 App 用着公共服务器的人，经常发来一封只有四个字的邮件：“不能用了？” 我觉得这是一个社会常识，花一分钟写的问题，不能期待一个毫无交情的陌生人花一个小时耐心地问你版本和操作步骤，模拟出你的环境来帮你分析解决。Windows 版加上 GFWList 功能以来，我反复呼吁给 GFWList 提交规则，但是一个月过去了竟然一个提交都没有。如果没有人做一点什么，它自己是不会更新的啊，没有人会义务地帮你打理这些。最近 net-speeder 又开始流行，害人害己。我觉得，政府无限的权力，都是大部分人自己放弃的。假货坑爹，让政府审核。孩子管不好，让政府关网吧。有人在微博骂我，让政府去删。房价太高，让政府去限购。我们的文化实在太独特，创造出了家长式威权政府，GFW 正是在这种背景下产生的，一个社会矛盾的终极调和器，最终生活不能自理的你每天做的每一件事情都要给政府审查一遍，以免伤害到其他同样生活不能自理的人。这是一个零和游戏，越和这样的用户打交道，越对未来持悲观态度，觉得 GFW 可能永远也不会消失，而墙内的这个局域网看起来还似乎生机勃勃的自成一体，真是让人绝望。顺便回答几个其它的问题吧。 为什么项目的文档和网站是英文的？开源项目用英文来维护本来就是一个约定俗成的做法。网络审查和屏蔽不是一个国家的事情，而是一个世界性的问题。这些项目有很多其它国家的用户，也有一些国外的大牛作出过宝贵的贡献。 参与的人只是凭兴趣做自己喜欢的事情，如果你的 issue 不像是一种思考和贡献，而更像是咨询客服，那被关掉了不要觉得惊讶。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[github CNAME 设置]]></title>
      <url>%2F2017%2F02%2F03%2FCNAME%E9%85%8D%E7%BD%AE%2F</url>
      <content type="text"><![CDATA[每次提交页面到githubpage上后总是受到一封警告邮件，虽说不影响访问但是被骚扰的很烦，(github的好意实在是太频繁了) The page build completed successfully, but returned the following warning:Your site’s DNS settings are using a custom subdomain, www.wenfanchao.win, that’s set up as an A record. We recommend you change this to a CNAME record pointing at wenfanchao.github.io. For more information, see https://help.github.com/pages/.For information on troubleshooting Jekyll see:https://help.github.com/articles/troubleshooting-jekyll-builds 有用的核心就是高亮的那句，意思很明确你的域名没有指向一个正确的地址 解决办法 首先在hexo的suorce中增加CNAME文件，内容就是你购买的域名 1www.wenfanchao.win 然后就在万网控制台修改A记录地址并增加CNAMECNAME的值就是你githubpage地址:wenfanchao.github.io 使用dig命令获取A记录的值1dig wenfanchao.github.io +nostats +nocomments +nocmd 注意wenfanchao.github.io为你博客的地址。 windows安装dig到百度云盘下载如下dig工具地址：链接：http://pan.baidu.com/s/1gedd9WB 密码：wtr3安装完毕后在环境变量中添加就能使用了]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hexo+githubpage安装配置]]></title>
      <url>%2F2017%2F02%2F03%2Fhexo%2Bgithub%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
      <content type="text"><![CDATA[最近终于用hexo+githubpage把个人博客建立了，中间遇到不少的坑，记下来省的再犯。 githubpagegithubpage真的是个好项目，为我们广大屌丝解决了很多问题具体的不想详细介绍不管是度娘还是谷哥都是一大堆。就给个地址: githubpage hexohexo也没啥说的官网巨详细基本就是照着做就可以地址:hexo themeshexo的社区很活跃提供的样式也很多这里选择了 next 这个被加星最多的样式，感觉还是很不错的该有的功能一应俱全 nextnext 主题功能全面，这里我只把个人修改的地方做个介绍 我修改了主题里的footer模板，原因是我对HEXO强力驱动和next主题支持这个标签实在是无爱了footer模板位置：themes\next\layout_partials\footer.swig模板很容易看懂，修改自己需要的内容就可以了。ps:注意里面变量的值是从你设置的对应语言的YML中获取的，我设置的简体中文。文件位置在themes/next/languages/zh-Hans.yml next的标签页面我不太喜欢，或者说是hexo的tagcloud样子我觉得都不是太爱。所以修改了样式，参考了freemind的标签样式 修改模板 themes\next\page.swig 替换了原有的标签生成修改为自定的 div与样式 12345678910111213141516&lt;div id="posts" class="posts-expand"&gt;&#123;% include '_partials/page-header.swig' %&#125; &#123;# tagcloud page support #&#125; &#123;% if page.type === "tags" %&#125; &lt;div class="tag-cloud"&gt; &lt;div class="tag-cloud-title"&gt; &#123;&#123; _p('counter.tag_cloud', site.tags.length) &#125;&#125; &lt;/div&gt; &lt;div class="widget"&gt; &lt;ul class="tag_box inline list-unstyled"&gt; &#123;% for item in site.tags %&#125; &lt;li&gt;&lt;a href="&#123;&#123;config.root&#125;&#125;&#123;&#123;item.path&#125;&#125;"&gt;&#123;&#123;item.name&#125;&#125;&lt;span&gt;&#123;&#123;item.posts.length&#125;&#125;&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; 修改样式 themes\next\source\css_common\components\post\post-tags.styl中增加 123456789101112131415161718192021222324252627282930313233343536373839.widget &#123; padding-bottom: 25px; border-bottom: 1px solid #e0e0e0;&#125;.tag_box &#123; margin:0; overflow:hidden;&#125;.tag_box li &#123; line-height:28px;&#125;.tag_box li i &#123; opacity:0.9;&#125;.tag_box.inline li &#123; float:left;&#125;.tag_box a &#123; padding: 2px 6px; margin: 2px; background: #e5e5e5; color:#555; border-radius: 3px; text-decoration:none; border:1px dashed #bbb;&#125;.tag_box a span&#123; vertical-align:super; font-size:0.8em;&#125;.tag_box a:hover &#123; background-color:#397bdd; color:#FFF;&#125;.tag_box a.active &#123; background:#57A957; border:1px solid #4C964D; color:#FFF;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hexo模板之ejs与Swig的优劣]]></title>
      <url>%2F2017%2F02%2F02%2Fhexo%E6%A8%A1%E6%9D%BF%E4%B9%8Bejs%E4%B8%8ESwig%E7%9A%84%E4%BC%98%E5%8A%A3%2F</url>
      <content type="text"><![CDATA[2个模板hexo刚入坑，随便从网上down了2个themes一个next，一个freemind 简单说一下认识ejs比swig语法更丰富当然这和2个模板的定位不一样,但是对我这种懒人来说EJS更好用为啥就举例一个 ejs1site.tags.random().limit(20) swig没有直接导致我要随机输出标签的功能完蛋。 swig只能老实的123&#123;% for item in site.tags %&#125; &lt;li&gt;&lt;a href=&quot;&#123;&#123;config.root&#125;&#125;&#123;&#123;item.path&#125;&#125;&quot;&gt;&#123;&#123;item.name&#125;&#125;&lt;span&gt;&#123;&#123;item.posts.length&#125;&#125;&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&#123;% endfor %&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2016%2F12%2F03%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new [layout] &lt;title&gt; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
    </entry>

    
  
  
</search>
