<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[oralce 闪回查询]]></title>
      <url>%2F2017%2F12%2F18%2Foracle%E5%91%BD%E4%BB%A4%E9%82%A3%E4%BA%9B%E4%BA%8B%2F</url>
      <content type="text"><![CDATA[一些平时总结的网上看到的内容，没事可以瞧瞧123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668/*创建表空间时指定的文件夹必须存在*/drop user snym cascade/*第1步：创建临时表空间 */create temporary tablespace ices_temptempfile 'D:\app\wenfanchao\oradata\oracle11g\ices_temp.dbf'size 50m autoextend on next 50m maxsize 20480m extent management local; /*第2步：创建数据表空间 */create tablespace ices_data logging datafile 'D:\app\wenfanchao\oradata\oracle11g\ices_data.dbf'size 50m autoextend on next 50m maxsize 20480m extent management local; /*第3步：创建用户并指定表空间 */create user ices identified by 111default tablespace ices_datatemporary tablespace ices_temp/*或使用默认表空间*//*default tablespace userstemporary tablespace temp*//*使用表空间配额无限制*/quota unlimited on ices_data;/*第4步：给用户授予权限*/grant connect,resource,dba to ices;grant execute on dbms_pipe to ices;导出exp ices/111@ICES file=d:\2012-7-15b.dmp full=yexp icesdb/icesdb@ICES2 file=d:\2012-7-6.dmp full=y查询锁表SELECT /*+ rule */ s.username,decode(l.type,'TM','TABLE LOCK','TX','ROW LOCK',NULL) LOCK_LEVEL,decode(l.LMODE,1,'空',2,'行级共享锁，其他对象只能查询这些数据行',3,'行级排它锁，在提交前不允许做DML操作',4,'共享锁',5,'共享行级排它锁',6,'排它锁'),o.owner,o.object_name,o.object_type,s.sid,s.serial#,s.terminal,s.machine,s.program,s.osuser,s.SQL_ADDRESS,s.SQL_HASH_VALUEFROM v$session s,v$lock l,dba_objects oWHERE l.sid = s.sidAND l.id1 = o.object_id(+)AND s.username is NOT Null ORDER BY LOCK_LEVEL --根据address 和 hash_value 查询对应语句select * from v$sql where address = '000000055B9A2728' and hash_value = 2183148849;--全语句select * from v$sqltext where address = '000000055B9A2728' and hash_value = 2183148849;--快速select s.sid,sql_text from v$session s,v$sqlarea a,v$locked_object b where s.sid=b.session_id and s.PREV_SQL_ADDR=a.address临时表被锁select sid,serial#from v$sessionwhere sid=(select sid from v$lock where id1 in (select object_id from user_objects where object_name='TMP_REP_5'));alter system kill session'72,146';杀锁命令alter system kill session 'sid,serial#'查看所有表空间位置select name from v$datafile;查看SQL性能SELECT BUFFER_GETS /*所有子游标运行这条语句导致的读内存次数*/ ,EXECUTIONS /*所有子游标的执行这条语句次数*/ ,SQL_TEXT FROM V$SQLAREA;查看消耗资源最多的SQL：Sql代码 SELECT hash_value, executions, buffer_gets, disk_reads, parse_calls FROM V$SQLAREA WHERE buffer_gets &gt; 10000000 OR disk_reads &gt; 1000000 ORDER BY buffer_gets + 100 * disk_reads DESC;选出最占用资源的查询Sql代码 select b.username username,a.disk_reads reads,a.executions exec, a.disk_reads/decode(a.executions,0,1,a.executions) rds_exec_ratio, a.sql_text statement from v$sqlarea a,dba_users b where a.parsing_user_id=b.user_id and username = 'SNYM'--sqlplus启动监听：lsnrctl start查看监听：lsnrctl status停止监听：lsnrctl stop--登录sqlplus 用户名/密码@监听名称 as 登录模式(normal/sysdba/sysoper)--1启动SQL*PLUS不与数据库连接 SQLPLUS /NOLOG2、以SYSDBA角色与Oracle连接 CONNECT username/password AS SYSDBA3、启动实例 1&gt;、启动一个实例，装配和打开一个数据库 STARTUP;或 STARTUP PFILE='d:/oracle/admin/mydb/scripts/initMYDB.ora'; 2&gt;、启动一个实例但不装配数据库 --典型应用为数据库创建 STARTUP NOMOUNT; 3&gt;、启动一个实例，装配数据库但不打开 --该模式下允许你执行特定的维护操作，包括 --重命名数据库、增删或重命名日志文件、启用和停用重做归档日志文件选项; --执行完整的数据库恢复 STARTUP MOUNT; 4&gt;、在启动时限制一个数据库的存取 --该模式使管理员可用数据库，但一般操作员不可用，应用于 --执行导入导出、执行SQL*loader的数据装载、临时阻止典型用户使用数据 --升迁或升级 --在正常模式下，有CREATE SESSION系统特权的用户可以连接到数据库 --限制模式下，同时具有CREATE SESSION和RESTRICTED SESSION两个系统特权的用户 --才可以存取数据 STARTUP RESTRICT; --关闭限制模式 ALTER SYSTEM DISABLE RESTRICTED SESSION; --将正在运行的正常模式改变为限制模式 ALTER SYSTEM ENABLE RESTRICTED SESSION; 5&gt;、强制一个实例启动（用于启动时出现问题情况，少用，慎用！） STARTUP FORCE; 6&gt;、启动实例，装配数据库和启动完全的介质恢复 STARTUP OPEN RECOVER;4、改变数据库可用性 1&gt;、装配数据库 ALTER DATABASE MOUNT; 2&gt;、打开数据库 ALTER DATABASE OPEN; 3&gt;、以只读或只写或读写方式打开数据库 ALTER DATABASE OPEN READ ONLY; ALTER DATABASE WRITE READ ONLY; ALTER DATABASE OPEN READ WRITE;&lt;二&gt;、ORACLE数据库关闭1、正常关闭 SHUTDOWN NORMAL；2、事务性关闭 --不许新连接，但等待现有的事务执行结束 SHUTDOWN TRANSACTIONAL；3、立即关闭 SHUTDOWN IMMEDIATE；4、强制关闭 --该模式关闭下次启动时要进行实例恢复过程 SHUTDOWN ABORT；5、停顿数据库 --该状态下只允许DBA会话，不允许新的非DBA连接建立 ALTER SYSTEM QUIESCE RESTRICTED; --取消停顿 ALTER SYSTEM UNQUIESCE; --查看实例的停顿状态 --列ACTIVE_STATE说明：NORMAL未停顿QUIESCING正在停顿，但仍有非DBA会话 --QUIESCED已停顿 select ACTIVE_STATE from V$INSTANCE;6、挂起数据库 --暂停对数据文件和控制文件的所有IO，可以在无IO干扰情况先进行备份 --挂起命令可以挂起数据库而并不指定一个实例 ALTER SYSTEM SUSPEND; --恢复到非挂起状态 ALTER SYSTEM RESUME; --查看挂起状态 SELECT DATABASE_STATUS FROM V$INSTANCE;--11G的DMP 导入 10G一、在11g服务器上，使用expdp命令备份数据11g 导出语句：EXPDP USERID='facial/facial@orcl as sysdba' schemas=facial directory=DATA_PUMP_DIR dumpfile=aa.dmp logfile=aa.log version=10.2.0.1.0其中，我的用户名是facial，密码是facial，数据库sid是orcl，schemas要导出的用户名是facial，要导入到 10.2.0.1.0版本的Oracle数据库中去。aa.dmp和aa.log将会在11g的dpdump目录中生成，例如我的11g装在了E盘下面，于是aa.dmp将会在E:\app\Administrator\admin\orcl\dpdump目录下被生成。二、在10g服务器上，使用impdp命令恢复数据准备工作：1.建库2.建表空间3.建用户并授权4.将aa.dmp拷贝到10g的dpdump目录下10g 导入语句：IMPDP USERID='facial/facial@orcl as sysdba' schemas=facial directory=DATA_PUMP_DIR dumpfile=aa.dmp logfile=aa.log version=10.2.0.1.0--给用户赋予创建临时表权限grant create any table to &lt;user&gt;-- 查询某个时间段的 SCN 系统快照号select timestamp_to_scn(to_timestamp('2012-07-19 15:00:00','YYYY-MM-DD HH24:MI:SS')) from dual;--根据系统快照号查询在该 SCN 号下 查询表对应的 数据集如select * from text1(表名) as of scn 25107913(SCN号);--通过 Insert into 语句还原表insert into text1(表名) select * from select * from text1(表名) as of scn 25107913(SCN号);select dbms_flashback.get_system_change_number from dual;就是oracle的会话数超出了限制，一般都是由于多次connect建立多个连接会话引起的，最后导致oracle无法响应新的请求，从而出现ora-12516错误。相关解决办法：1.查看当前连接进程数SQL&gt;select count(*) from v$process;2.查看连接数上限SQL&gt;select value from v$parameter where name = 'processes';3.查看当前数据库的processes设置SQL&gt; show parameter processesNAME TYPE VALUEdb_writer_processes integer 1gcs_server_processes integer 0job_queue_processes integer 10log_archive_max_processes integer 2processes integer 150SQL&gt; show parameter sessionsNAME TYPE VALUEjava_soft_sessionspace_limit integer 0license_max_sessions integer 0license_sessions_warning integer 0logmnr_max_persistent_sessions integer 1sessions integer 170shared_server_sessions integer4.只要会话连接数超过上面的process数150或者sessions数170，再来一个的会话进程，就会产生12516错误。因此可以修改一下该值：sessions=1.1*processes+5;//这个是sessions值和processes值的关系，最好按照这样做，因此如果要将processes数设置为500，则sessions数必须为1.1*500+5=555SQL&gt; alter system set processes=500 scope=spfile;系统已更改。SQL&gt; alter system set sessions=555 scope=spfile;系统已更改。1。会话特有的临时表CREATE GLOBAL TEMPORARY &lt;TABLE_NAME&gt; (&lt;column specification&gt;)ON COMMIT PRESERVE ROWS；2。事务特有的临时表CREATE GLOBAL TEMPORARY &lt;TABLE_NAME&gt; (&lt;column specification&gt;)ON COMMIT DELETE ROWS；CREATE GLOBAL TEMPORARY TABLE MyTempTableselect * from DBA_USERS;SELECT * FROM ALL_USERS;---create temporary tablespace impr_temp tempfile '+ASM_DATA/snym/tempfile/impr_temp.dbf' size 10m autoextend on next 10m maxsize unlimited;create tablespace impr_idata logging datafile '+ASM_DATA/snym/datafile/impr_idata.dbf' size 20m autoextend on next 20m maxsize unlimited;create user impr identified by impr123456 default tablespace impr_idata temporary tablespace impr_temp;grant connect,resource,dba to impr;create temporary tablespace snym_temp tempfile '+ASM_DATA/snym/tempfile/snym_temp.dbf' size 10m autoextend on next 10m maxsize unlimited;create tablespace snym_idata logging datafile '+ASM_DATA/snym/datafile/snym_idata.dbf' size 20m autoextend on next 20m maxsize unlimited;--创建数据链create public database link DB63 connect to 用户名 identified by "密码" using '(DESCRIPTION = (ADDRESS_LIST = (ADDRESS = (PROTOCOL = TCP)(HOST = 10.201.8.63)(PORT = 1521)) ) (CONNECT_DATA = (SERVICE_NAME = orcl) ) )';--创建同义词create public synonym test_syn for scott.test2@DB104---解决提示动态执行表不可访问用SYS登录，授权给相应的用户，用下面的语句，grant select on v_$statname to user;自动统计功能可以关掉。最后的这个，超时断开问题，可以设置USER_PROFILES里面看到的IDLE_TIME值，首先查看你的当前用户的PROFILE是哪个，select profile from dba_users where username='SCOTT' ;假如是DEFAULT这个PROFILE，那么修改IDLE_TIME这个值alter profile default limit idle_time unlimited ;必要时创建新的PROFILE。--实际应用中是不能对远程的表 使用递归语句的 但是可以通过使远程表与本地表产生笛卡尔积的方式变相解决使用递归语句-------------------------------------------------------oracle赋权------------------------------------------我们将通过介绍命令的方式，谈谈Oracle用户权限表的管理方法，希望对大家有所帮助。 我们将从创建Oracle用户权限表开始谈起，然后讲解登陆等一般性动作，使大家对Oracle用户权限表有个深入的了解。 一、创建 sys;//系统管理员，拥有最高权限 system;//本地管理员，次高权限 scott;//普通用户，密码默认为tiger,默认未解锁 sys;//系统管理员，拥有最高权限 system;//本地管理员，次高权限 scott;//普通用户，密码默认为tiger,默认未解锁 二、登陆 sqlplus / as sysdba;//登陆sys帐户 sqlplus sys as sysdba;//同上 sqlplus scott/tiger;//登陆普通用户scott sqlplus / as sysdba;//登陆sys帐户 sqlplus sys as sysdba;//同上 sqlplus scott/tiger;//登陆普通用户scott 三、管理用户 create user zhangsan;//在管理员帐户下，创建用户zhangsan alert user scott identified by tiger;//修改密码 create user zhangsan;//在管理员帐户下，创建用户zhangsan alert user scott identified by tiger;//修改密码 四，授予权限 1、默认的普通用户scott默认未解锁，不能进行那个使用，新建的用户也没有任何权限，必须授予权限 /*管理员授权*/ grant create session to zhangsan;//授予zhangsan用户创建session的权限，即登陆权限 grant unlimited session to zhangsan;//授予zhangsan用户使用表空间的权限 grant create table to zhangsan;//授予创建表的权限 grante drop table to zhangsan;//授予删除表的权限 grant insert table to zhangsan;//插入表的权限 grant update table to zhangsan;//修改表的权限 grant all to public;//这条比较重要，授予所有权限(all)给所有用户(public) /*管理员授权*/ grant create session to zhangsan;//授予zhangsan用户创建session的权限，即登陆权限 grant unlimited session to zhangsan;//授予zhangsan用户使用表空间的权限 grant create table to zhangsan;//授予创建表的权限 grante drop table to zhangsan;//授予删除表的权限 grant insert table to zhangsan;//插入表的权限 grant update table to zhangsan;//修改表的权限 grant all to public;//这条比较重要，授予所有权限(all)给所有用户(public) 2、oralce对权限管理比较严谨，普通用户之间也是默认不能互相访问的，需要互相授权 /*oralce对权限管理比较严谨，普通用户之间也是默认不能互相访问的*/ grant select on tablename to zhangsan;//授予zhangsan用户查看指定表的权限 grant drop on tablename to zhangsan;//授予删除表的权限 grant insert on tablename to zhangsan;//授予插入的权限 grant update on tablename to zhangsan;//授予修改表的权限 grant insert(id) on tablename to zhangsan; grant update(id) on tablename to zhangsan;//授予对指定表特定字段的插入和修改权限，注意，只能是insert和update grant alert all table to zhangsan;//授予zhangsan用户alert任意表的权限 /*oralce对权限管理比较严谨，普通用户之间也是默认不能互相访问的*/ grant select on tablename to zhangsan;//授予zhangsan用户查看指定表的权限 grant drop on tablename to zhangsan;//授予删除表的权限 grant insert on tablename to zhangsan;//授予插入的权限 grant update on tablename to zhangsan;//授予修改表的权限 grant insert(id) on tablename to zhangsan; grant update(id) on tablename to zhangsan;//授予对指定表特定字段的插入和修改权限，注意，只能是insert和update grant alert all table to zhangsan;//授予zhangsan用户alert任意表的权限 五、撤销权限 基本语法同grant,关键字为revoke 基本语法同grant,关键字为revoke 六、查看权限 select * from user_sys_privs;//查看当前用户所有权限 select * from user_tab_privs;//查看所用用户对表的权限 select * from user_sys_privs;//查看当前用户所有权限 select * from user_tab_privs;//查看所用用户对表的权限 七、操作表的用户的表 /*需要在表名前加上用户名，如下*/ select * from zhangsan.tablename /*需要在表名前加上用户名，如下*/ select * from zhangsan.tablename 八、权限传递 即用户A将权限授予B，B可以将操作的权限再授予C，命令如下： grant alert table on tablename to zhangsan with admin option;//关键字 with admin option grant alert table on tablename to zhangsan with grant option;//关键字 with grant option效果和admin类似 grant alert table on tablename to zhangsan with admin option;//关键字 with admin option grant alert table on tablename to zhangsan with grant option;//关键字 with grant option效果和admin类似 九、角色 角色即权限的集合，可以把一个角色授予给用户 create role myrole;//创建角色 grant create session to myrole;//将创建session的权限授予myrole grant myrole to zhangsan;//授予zhangsan用户myrole的角色 drop role myrole;删除角色 /*但是有些权限是不能授予给角色的，比如unlimited tablespace和any关键字*/--建立JOBdeclare v_jobno number; begin dbms_job.submit(v_jobno,'prc_fistone(1);',sysdate,'sysdate+1'); dbms_output.put_line('号：'|| v_jobno); commit; end; --修改 begin sys.dbms_job.change(job =&gt; 23, what =&gt; 'prc_nextyear(1);', next_date =&gt; to_date('06-03-2013', 'dd-mm-yyyy'), interval =&gt; 'sysdate+1'); commit;end;-------------------------------------------------------------------------------------------我们在确定应用程序性能的时候，更多地会关注其中SQL语句的执行情况。通常应用的性能瓶颈会在数据库这边，因此数据库的sql语句是我们优化的重点。要对sql语句进行优化，首先要知道应用程序执行了哪些SQL语句。下面我介绍三种方法来获得应用程序所执行的SQL语句。方法一：如果你采用的数据库是oracle的话，那么可以利用oracle本身的10046事件跟踪SQL语句。步骤如下：1、首先获得spid、sid、serial#，machine为你的机器名 SQL&gt; select b.spid,a.sid,a.serial#, a.machine from v$session a,v$process b where a.paddr = b.addr and a.machine='yz'; SPID SID SERIAL# MACHINE 9497 49 3406 yz 2、利用10046事件开始跟踪 SQL&gt;execute sys.dbms_system.set_ev(49, 3406,10046,1,''); PL/SQL procedure successfully completed. 3、这时候你可以运行应用程序，对于web 应用你就可以打开你认为性能比较差页面。4、如果你要查看这段时间执行了哪些sql语句，可以执行下面的语句结束跟踪: SQL&gt;execute sys.dbms_system.set_ev(49,3406,10046,0,''); PL/SQL procedure successfully completed. 5、SQL trace 工具会收集这个过程中执行的SQL的性能状态数据，记录到一个跟踪文件中.这个跟踪文件提供了许多有用的信息,例如解析次数.执行次数,CPU使用时间等。6、这时候你可以通过下面的语句获得产生的跟踪文件所在的目录: SQL&gt; select value from v$parameter where name = 'user_dump_dest'; VALUE /opt/oracle/admin/ocn/udump 7、在/opt/oracle/admin/ocn/udump下找到yzoracle_ora_9497.trc。9497是你当前应用的spid。8、注意yzoracle_ora_9497.trc是不可读的，我们需要执行oracle的tkprof命令，将yzoracle_ora_9497.trc转化为我们可读的文本文件。 tkprof yzoracle_ora_9497.trc yzoracle_ora_9497.sql 这样你就可以在yzoracle_ora_9497.sql文件中看到所有的sql语句执行次数,CPU使用时间等数据。------------------------------------------------------------------------------------修改Oracle 11g账户过期时间开发过程中不需要限制账户的过期时间，所以需要进行一些设置。1 查看用户的proifle是哪个，一般是default：SELECT username,PROFILE FROM dba_users;2 查看指定概要文件（如default）的密码有效期设置：SELECT * FROM dba_profiles s WHERE s.profile='DEFAULT' AND resource_name='PASSWORD_LIFE_TIME'; www.2cto.com 3 将密码有效期由默认的180天修改成“无限制”：ALTER PROFILE DEFAULT LIMIT PASSWORD_LIFE_TIME UNLIMITED;修改之后不需要重启动数据库，会立即生效。4 修改后，还没有被提示ORA-28002警告的帐户不会再碰到同样的提示；已经被提示的帐户必须再改一次密码，举例如下：$sqlplus / as sysdbasql&gt; alter user smsc identified by &lt;原来的密码&gt; ----不用换新密码oracle11g启动参数resource_limit无论设置为false还是true，密码有效期都是生效的，所以必须通过以上方式进行修改。以上的帐户名请根据实际使用的帐户名更改。-----------------发现的问题----------------oracle 不能删除没有主外键关系的视图数据如 A,B两表 没有主外键关系则delete from (select * from a,b where a.id = b.id)会产生 ora-01752 错误-------------------------------------------]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[oralce 闪回查询]]></title>
      <url>%2F2017%2F12%2F18%2Foracle_scn%2F</url>
      <content type="text"><![CDATA[闪回查询的使用1234567-- 查询某个时间段的 SCN 系统快照号 select timestamp_to_scn(to_timestamp('2012-07-19 15:00:00','YYYY-MM-DD HH24:MI:SS')) from dual; --根据系统快照号查询在该 SCN 号下 查询表对应的 数据集如 select * from text1(表名) as of scn 25107913(SCN号); --通过 Insert into 语句还原表 insert into text1(表名) select * from select * from text1(表名) as of scn 25107913(SCN号); select dbms_flashback.get_system_change_number from dual;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[关于生活的感想]]></title>
      <url>%2F2017%2F10%2F09%2F%E5%85%B3%E4%BA%8E%E7%94%9F%E6%B4%BB%E7%9A%84%E6%84%9F%E6%83%B3%2F</url>
      <content type="text"><![CDATA[当前生活 生活生活，生下来就要干活。 我自己的生活状态]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[javascript之file文件上传]]></title>
      <url>%2F2017%2F05%2F25%2Fjavascript%E4%B9%8Bfile%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%2F</url>
      <content type="text"><![CDATA[使用JS formData 格式提文件数据XMLHttpRequest Level 2添加了一个新的接口FormData.利用FormData对象,我们可以通过JavaScript用一些键值对来模拟一系列表单控件,我们还可以使用XMLHttpRequest的send()方法来异步的提交这个”表单”.比起普通的ajax,使用FormData的最大优点就是我们可以异步上传一个二进制文件. 使用Jquery.ajax方法提交file文件123456789101112131415161718192021222324252627function uploadFile() &#123; var pic = $("#pic").get(0).files[0]; var formData = new FormData(); formData.append("file", pic); /** * 必须false才会避开jQuery对 formdata 的默认处理 * XMLHttpRequest会对 formdata 进行正确的处理 */ $.ajax(&#123; type: "POST", url: "appleid.txt", data: formData, processData: false, //必须false才会自动加上正确的Content-Type contentType: false, xhr: function() &#123; var xhr = $.ajaxSettings.xhr(); if (onprogress &amp;&amp; xhr.upload) &#123; xhr.upload.addEventListener("progress", onprogress, false); return xhr; &#125; &#125; &#125;); &#125; /** * 侦查附件上传情况 ,这个方法大概0.05-0.1秒执行一次 */ function onprogress(evt) &#123; var loaded = evt.loaded; //已经上传大小情况 var tot = evt.total; //附件总大小 var per = Math.floor(100 * loaded / tot); //已经上传的百分比 $("#son").html(per + "%"); $("#son").css("width", per + "%"); &#125; 使用浏览器打开摄像头，拍照后上传图片到服务器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111var canvas = document.getElementById("canvas"); var context = canvas.getContext("2d") // 标准的API var p = navigator.mediaDevices.getUserMedia(&#123; audio: true, video: true &#125;); //调用方法给video赋值达到摄像头调用 p.then(function(mediaStream) &#123; var video = document.getElementById("video"); video.src = window.URL.createObjectURL(mediaStream); video.onloadedmetadata = function(e) &#123; // Do something with the video here. &#125;; &#125;); p.catch(function(err) &#123; console.log(err.name); &#125;); // always check for errors at the end. jQuery(document).ready(function($) &#123; &#125;); $("#uploadFile").on('change',function()&#123; $(this).next().html($("#uploadFile").val()); &#125;); $("#uploadFile").on('click',function()&#123; $("#picTip").empty(); $("#son").empty(); &#125;); $("#btnKacha").on('click',function()&#123; context.drawImage(video, 0, 0, 640, 480); &#125;); //这2个方法是处理拍摄类型的字节流 function base64DecToArr(e, t) &#123; var a = e.replace(/[^A-Za-z0-9\+\/]/g, "") , i = a.length , s = t ? Math.ceil((i * 3 + 1 &gt;&gt; 2) / t) * t : i * 3 + 1 &gt;&gt; 2 , r = new Uint8Array(s); for (var o, n, h = 0, l = 0, c = 0; c &lt; i; c++) &#123; n = c &amp; 3; h |= b64ToUint6(a.charCodeAt(c)) &lt;&lt; 18 - 6 * n; if (n === 3 || i - c === 1) &#123; for (o = 0; o &lt; 3 &amp;&amp; l &lt; s; o++, l++) &#123; r[l] = h &gt;&gt;&gt; (16 &gt;&gt;&gt; o &amp; 24) &amp; 255 &#125; h = 0 &#125; &#125; return r &#125; function b64ToUint6(e) &#123; return e &gt; 64 &amp;&amp; e &lt; 91 ? e - 65 : e &gt; 96 &amp;&amp; e &lt; 123 ? e - 71 : e &gt; 47 &amp;&amp; e &lt; 58 ? e + 4 : e === 43 ? 62 : e === 47 ? 63 : 0 &#125; //接受BASE64的字符串 var h = new Blob([base64DecToArr("rrressd")],&#123; type: "image/png" &#125;); $("#btnUpload").on('click',function()&#123; var filebase64 = canvas.toDataURL("image/png"); var formData = new FormData(); formData.append("file",h,"321.png"); formData.append("uuid",123); $.ajax(&#123; url : 'manager/upload', type : 'POST', data : formData, datatype:'json', // 告诉jQuery不要去处理发送的数据 processData : false, // 告诉jQuery不要去设置Content-Type请求头 contentType : false, xhr: function() &#123; var xhr = $.ajaxSettings.xhr(); if (onprogress &amp;&amp; xhr.upload) &#123; xhr.upload.addEventListener("progress", onprogress, false); return xhr; &#125; &#125;, beforeSend:function()&#123; $(".blockdiv").show(); &#125;, success : function(data) &#123; $(".blockdiv").hide(); var result = JSON.parse(data); if(result.msg === "ok" )&#123; alert("上传成功！"); var imgw = result.imgWidth*0.6; var html = '&lt;img style="width: '+imgw+'px;height: auto;max-width: 300px;min-width: 100px;" src="&lt;%=basePath%&gt;upload/'+result.imgpath + '?' + (new Date().getTime()) +'" /&gt;'; console.log(html); $("#imgpath&gt;img").remove(); $("#imgpath").append(html); &#125;else&#123; alert("上传失败！"); &#125; &#125;, error : function(responseStr) &#123; $(".blockdiv").hide(); alert("上传失败！"); &#125; &#125;); &#125;); /** * 侦查附件上传情况 ,这个方法大概0.05-0.1秒执行一次 */ function onprogress(evt) &#123; var loaded = evt.loaded; //已经上传大小情况 var tot = evt.total; //附件总大小 var per = Math.floor(100 * loaded / tot); //已经上传的百分比 $("#son").html(per + "%"); &#125; 对应的HTML代码12345678&lt;video id="video" width="640" height="480" autoplay=""&gt;&lt;/video&gt; &lt;button id="btnKacha"&gt;拍照&lt;/button&gt; &lt;button id="btnUpload"&gt;上传&lt;/button&gt; &lt;canvas id="canvas" width="640" height="480"&gt;&lt;/canvas&gt; &lt;div id = "picTip"&gt;&lt;/div&gt; &lt;div id = "imgpath"&gt;&lt;/div&gt; &lt;div class="blockdiv"&gt;&lt;span&gt;正在上传&lt;/span&gt;&lt;/div&gt; &lt;div id="son"&gt;&lt;/div&gt;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[memcathed+tomcat7实现session共享搭建环境与问题解决]]></title>
      <url>%2F2017%2F04%2F06%2Fmemcathed%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%E5%8F%8A%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
      <content type="text"><![CDATA[memcathed 下载安装1. 下载安装包，下载地址1.4.36 2. linux下安装 memcached2.1 安装libevent-2.0.15 安装之前，需要先确认系统中是否有libevent，因为memcached依赖这个包。查看:命令： rpm -qa|grep libevent显示的包：libevent-1.4.13-4.el6.x86_64此时，系统已经安装过了，需要卸载重新下载安装。卸载：rpm -e libevent-1.4.13-4.el6.x86_64 下载libevent，并安装 下载地址 解压：tar -zxvf libevent-2.0.21-stable.tar.gz安装：./configure –prefix=/usr 编译：makemake install安装完成！2.2 下载memcached,并解压命令： tar -zxvf memcached-1.4.36.tar.gz 安装：./configure –with-libevent=/usr若安装过程中提示找不到libevent路径时，使用–with-libevent=libevent安装的目录./configure –prefix=/usr –with-libevent=/usr编译：makemake install安装完成！ 启动/usr/local/bin/memcached -d -m 10 -u root -l 192.168.158.135 -p 11211 -c 256 -P /usr/memcached.pid启动参数说明：-d 选项是启动一个守护进程。-u root 表示启动memcached的用户为root。-m 是分配给Memcache使用的内存数量，单位是MB，默认64MB。-M return error on memory exhausted (rather than removing items)。-u 是运行Memcache的用户，如果当前为root 的话，需要使用此参数指定用户。-p 是设置Memcache的TCP监听的端口，最好是1024以上的端口。-c 选项是最大运行的并发连接数，默认是1024。-P 是设置保存Memcache的pid文件。 停止kill cat /usr/memcached.pid 或者先查看进程的idps -ef|grep memcachedroot 15144 1 0 08:43 ? 00:00:00/usr/local/bin/memcached -d -m 10 -u root -l 192.168.158.135 -p 11211 -c 256 -P /usr/memcached.pid15144为pid停止命令为：kill -9 15144 3 配置tomcat7 msm的配置可以参考google官方说明，但官方推荐的JAR包版本过老且缺少依赖如果使用TOMCAT7.0.60以上的版本会出现问题。如出现: tomcat启动正常，访问时出现异常，信息：http11.AbstractHttp11Processor process java.lang.NoSuchFieldError: attributes处理：修改为粘性（sticky）方式可以访问（这里官方文档还有问题在只有1个节点时，不能指定failoverNodes=”n1” ），但因为使用nginx，并且nginx非最外层，客户的ip是动态的等原因不能是ip_hash进行处理，存在多台tomcat，必须使用非粘性（non-sticky）经过查找终于发现msm1.9.3之后fix了这个问题，所以进行版本升级：添加memcached和msm(Memcached_Session_Manager)的依赖jar包，如下12345678910spymemcached-2.12.0.jarreflectasm-1.10.1.jarobjenesis-2.1.jarmsm-kryo-serializer-1.9.5.jarminlog-1.3.0.jarmemcached-session-manager-tc7-1.9.5.jarmemcached-session-manager-1.9.5.jarkryo-serializers-0.37.jarkryo-3.0.3.jarasm-5.0.3.jar 修改tomcat7下的 conf/context.xml12345678&lt;Manager className="de.javakaffee.web.msm.MemcachedBackupSessionManager"memcachedNodes="n1:wwww.demo.com:11211,n2:www.demo2.com:11211"sticky="false"sessionBackupAsync="false"sessionBackupTimeout="1000"lockingMode="uriPattern:/path1|/path2"requestUriIgnorePattern=".*\.(ico|png|gif|jpg|css|js)$"transcoderFactoryClass="de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory" /&gt; 测试是否实现session共享对2个tomcat进行相同配置后可分别向2个tomcat中增加项目AA中存放一个用来测试的DEMO.JSP内容如下12345 SessionID:&lt;%=session.getId()%&gt;&lt;br&gt;SessionIP:&lt;%=request.getServerName()%&gt;&lt;br&gt;SessionPort:&lt;%=request.getServerPort()%&gt; 刷新2个tomcat 项目下的demo页面发现SESSION 相同则配置成功。 注意问题如果在一台机器上同时启动2个tomcat，一个配置了memcathed一个使用本地内存来储存SESSION,2个项目同时启动并运行同一个项目时，会导致memcathed+tomcat 的环境错误导致memcathed断开，SESSION不断变化。但这种情况与正常的memcathed服务器掉线导致的断开session变化不同。因为没有配置memcathed的tomcat启动后也向页面写入了cookie这样会导致使用memcathed管理session的tomcat项目分配的cookie会始终与memcathed服务端记录的cookie不同导致项目无法再次重新连接到memcathed。解决办法，清除浏览器cookie。为避免上述问题出现，可以对tomcat进行配置时每个tomcat一台机器上的tomcat单独使用一块内存。对每一个tomcat设置独立的区域]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用supermap把矢量地图转换为svg地图]]></title>
      <url>%2F2017%2F03%2F03%2F%E4%BD%BF%E7%94%A8supermap%E6%8A%8A%E7%9F%A2%E9%87%8F%E5%9C%B0%E5%9B%BE%E8%BD%AC%E6%8D%A2%E4%B8%BAsvg%E5%9C%B0%E5%9B%BE%2F</url>
      <content type="text"><![CDATA[supermap to svg 最近看到了一个把矢量地图制作成SVG地图的功能，个人觉得挺花哨的研究了一下，把研究经验分享一下，如果有更好的方法请跟我联系大家可以探讨 使用supermap idesktop 把一个线对象导出为 Autocad dxf 文件 使用工具DWG to SVG Converter MX 把 dxf 转换为 svg 文件 使用工具Inkscape 给闭环的线填充内容获取填充后的 svg 图形 使用 Raphael js 绘制svg添加效果]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[javascript小数运算]]></title>
      <url>%2F2017%2F02%2F22%2Fjavascript%E5%B0%8F%E6%95%B0%E8%BF%90%E7%AE%97%2F</url>
      <content type="text"><![CDATA[javascript 小数运算 12console.log(0.1 + 0.2); //0.30000000000000004console.log(0.1 + 0.2 == 0.3); //false JavaScript 中的 number 类型就是浮点型，JavaScript 中的浮点数采用IEEE-754 格式的规定，这是一种二进制表示法，可以精确地表示分数，比如1/2，1/8，1/1024，每个浮点数占64位。但是，二进制浮点数表示法并不能精确的表示类似0.1这样 的简单的数字，会有舍入误差。 由于采用二进制，JavaScript 也不能有限表示 1/10、1/2 等这样的分数。在二进制中，1/10(0.1)被表示为 0.00110011001100110011…… 注意 0011 是无限重复的，这是舍入误差造成的，所以对于 0.1 + 0.2 这样的运算，操作数会先被转成二进制，然后再计算： 0.1 =&gt; 0.0001 1001 1001 1001…（无限循环） 0.2 =&gt; 0.0011 0011 0011 0011…（无限循环） 双精度浮点数的小数部分最多支持 52 位，所以两者相加之后得到这么一串 0.0100110011001100110011001100110011001100…因浮点数小数位的限制而截断的二进制数字，这时候，再把它转换为十进制，就成了 0.30000000000000004。 对于保证浮点数计算的正确性，有两种常见方式。 一是先升幂再降幂： 12345678910function add(num1, num2)&#123; let r1, r2, m; r1 = (''+num1).split('.')[1].length; r2 = (''+num2).split('.')[1].length; m = Math.pow(10,Math.max(r1,r2)); return (num1 * m + num2 * m) / m;&#125;console.log(add(0.1,0.2)); //0.3console.log(add(0.15,0.2256)); //0.3756 二是是使用内置的 toPrecision() 和 toFixed() 方法，注意，方法的返回值字符串。 1234function add(x, y) &#123; return x.toPrecision() + y.toPrecision()&#125;console.log(add(0.1,0.2)); //"0.10.2"]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hive安装配置使用]]></title>
      <url>%2F2017%2F02%2F08%2Fhive%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[HIVE简介 HIVE下载，安装下载下载地址 安装安装MYSQL1234#CentOSyum install mysql mysql-connector-java#启动Mysqlservice mysqld start 安装HIVE1tar -zxvf apache-hive-2.1.0-bin.tar.gz -C usr/hadoop HIVE配置配置文件hive-env.sh|hive-stie.xml hive-env.sh 1HADOOP_HOME=/etc/hadoop/hadoop-2.6.0 hive-stie.xmlss 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file. --&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;&lt;!--metastore的端口--&gt;&lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://hadoop:9083&lt;/value&gt;&lt;/property&gt;&lt;!--HiveServer2的端口--&gt;&lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;beeline.hs2.connection.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;beeline.hs2.connection.password&lt;/name&gt; &lt;value&gt;111111&lt;/value&gt; &lt;/property&gt; &lt;!--使用mysql存储元数据--&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://127.0.0.1/metastore?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;sa&lt;/value&gt; &lt;/property&gt; &lt;!--hive在HDFS上的存储路径--&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop:9000&lt;/value&gt; &lt;/property&gt; &lt;!--此外我们还配置自动创建Meta Store的数据库和表--&gt; &lt;property&gt; &lt;name&gt;datanucleus.autoCreateSchema&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;datanucleus.autoStartMechanism&lt;/name&gt; &lt;value&gt;SchemaTable&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;datanucleus.schema.autoCreateTables&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; HIVE使用启动 启动hive组件 1234#启动MetaStore Servernohup hive --service metastore &gt;&gt; /usr/hadoop/apache-hive-2.1.0-bin/logs/metastore.log 2&gt;&amp;1 &amp;#启动HiveServer2nohup hive --service hiveserver2 &gt;&gt; /usr/hadoop/apache-hive-2.1.0-bin/logs/hive.log 2&gt;&amp;1 &amp; hive启动hive beeline启动12beelinebeeline&gt; !connect jdbc:hive2://localhost:10000/default root 111111 建表123456789create table if not exists user_dimension (uid STRING,name STRING,gender STRING,birth DATE,province STRING)ROW FORMAT DELIMITEDFIELDS TERMINATED BY ','&lt;!--创建完后 可以使用 shwo create tabel &lt;tablename&gt; 查看详细的建表过程。--&gt; 导入数据1load data inpath '/flume/record/2017-01-06/0500' overwrite into table &lt;tablename&gt;; 执行SQL脚本1hive -f /usr/downgit/hadooptraining/hive/command/create_table_record.sql 导出数据到HDFS123exportinsert overwrite directory '/demo'select sum(price),source_province from record group by source_province;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[flume安装配置使用]]></title>
      <url>%2F2017%2F02%2F08%2Fflume%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[FLUME介绍flume是一个分布式的，可靠的，可用的服务，有效地收集，汇总和移动大量的日志数据。它有一个简单的和灵活的架构基于流数据流。具有很好的健壮性和容错性具有可调性可靠性机制和多故障转移和恢复机制。它使用一个简单的可扩展数据模型，允许联机分析应用程序。 flume下载flume直接在官网提供最新版本下载，同时也提供了历史版本下载 flume安装 下载flume后直接解压到文件夹 1$ tar -zxvf apache-flume-1.7.0-bin.tar.gz -C usr/hadoop/ 修改配置文件 1flume-env.ps1.template flume-env.ps1.sh 修改文件内容 12export JAVA_HOME=/usr/java/jdk1.8.0_111FLUME_CLASSPATH="$HADOOP_HOME/share/hadoop/common/hadoop-common-2.6.0.jar" 修改profile文件增加flume 12export FLUME_HOME=/etc/hadoop/apache-flume-1.7.0-binexport PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$FLUME_HOME/bin fulme使用 监听一个文件夹有新文件进入后导入到HDFS中 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# Licensed to the Apache Software Foundation (ASF) under one# or more contributor license agreements. See the NOTICE file# distributed with this work for additional information# regarding copyright ownership. The ASF licenses this file# to you under the Apache License, Version 2.0 (the# "License"); you may not use this file except in compliance# with the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing,# software distributed under the License is distributed on an# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY# KIND, either express or implied. See the License for the# specific language governing permissions and limitations# under the License.# The configuration file needs to define the sources,# the channels and the sinks.# Sources, channels and sinks are defined per agent,# in this case called 'agent'logAgent.sources = logSourcelogAgent.channels = fileChannellogAgent.sinks = hdfsSink# For each one of the sources, the type is definedlogAgent.sources.logSource.type = spooldir#监听的文件夹logAgent.sources.logSource.spoolDir = /root/bigdata/datasourcelogAgent.sources.logSource.basenameHeader = truelogAgent.sources.logSource.basenameHeaderKey = basename# The channel can be defined as follows.logAgent.sources.logSource.channels = fileChannel# Each sink's type must be definedlogAgent.sinks.hdfsSink.type = hdfs#导入HDFS的位置logAgent.sinks.hdfsSink.hdfs.path = hdfs://hadoop:9000/flume/weatherlogAgent.sinks.hdfsSink.hdfs.filePrefix= %&#123;basename&#125;logAgent.sinks.hdfsSink.hdfs.rollInterval= 600logAgent.sinks.hdfsSink.hdfs.rollCount= 10000logAgent.sinks.hdfsSink.hdfs.rollSize= 0logAgent.sinks.hdfsSink.hdfs.round = truelogAgent.sinks.hdfsSink.hdfs.roundValue = 1logAgent.sinks.hdfsSink.hdfs.roundUnit = minutelogAgent.sinks.hdfsSink.hdfs.fileType = DataStreamlogAgent.sinks.hdfsSink.hdfs.useLocalTimeStamp = true#Specify the channel the sink should uselogAgent.sinks.hdfsSink.channel = fileChannel# Each channel's type is defined.logAgent.channels.fileChannel.type = filelogAgent.channels.fileChannel.checkpointDir= /etc/hadoop/apache-flume-1.7.0-bin/dataCheckpointDirlogAgent.channels.fileChannel.dataDirs= /etc/hadoop/apache-flume-1.7.0-bin/dataDir# Other config values specific to each type of channel(sink or source)# can be defined as well# In this case, it specifies the capacity of the memory channel 启动脚本 1flume-ng agent --conf /etc/hadoop/apache-flume-1.7.0-bin/conf/ --conf-file /etc/hadoop/apache-flume-1.7.0-bin/conf/flume-conf-weather.properties --name logAgent -Dflume.root.logger=DEBUG,console -Dflume.monitoring.type=http -Dflume.monitoring.port=34545]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[sqoop安装配置使用]]></title>
      <url>%2F2017%2F02%2F07%2Fsqoop%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[sqoop介绍Sqoop(发音：skup)是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。 sqoop下载，安装下载下载地址 安装下载好后解压文件1$ tar -zxvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C usr/sqoop sqoop配置修改 $sqoop_home/conf 下的模板文件1mv sqoop-env-template.cmd sqoop-env.sh 增加export HADOOP_COMMON_HOME=/etc/hadoop/hadoop-2.6.0 sqoop导入导出语句从数据库导入到HDFS1sqoop import --connect jdbc:mysql://117.37.36.199:3306/log --username root --password root --table user_dimension --driver com.mysql.jdbc.Driver --m 10 --target-dir /warehouse/user_dimension 参数 说明 –append 将数据追加到hdfs中已经存在的dataset中。使用该参数，sqoop将把数据先导入到一个临时目录中，然后重新给文件命名到一个正式的目录中，以避免和该目录中已存在的文件重名。 –as-avrodatafile 将数据导入到一个Avro数据文件中 –as-sequencefile 将数据导入到一个sequence文件中 –as-textfile 将数据导入到一个普通文本文件中，生成该文本文件后，可以在hive中通过sql语句查询出结果。 –boundary-query 边界查询，也就是在导入前先通过SQL查询得到一个结果集，然后导入的数据就是该结果集内的数据，格式如：–boundary-query ‘select id,creationdate from person where id = 3’，表示导入的数据为id=3的记录，或者select min(), max() from ，注意查询的字段中不能有数据类型为字符串的字段，否则会报错：java.sql.SQLException: Invalid value for getLong() 目前问题原因还未知 –columns 指定要导入的字段值，格式如：–columns id,username –direct 直接导入模式，使用的是关系数据库自带的导入导出工具。官网上是说这样导入会更快 –direct-split-size 在使用上面direct直接导入的基础上，对导入的流按字节数分块，特别是使用直连模式从PostgreSQL导入数据的时候，可以将一个到达设定大小的文件分为几个独立的文件。 –inline-lob-limit 设定大对象数据类型的最大值 -m,–num-mappers 启动N个map来并行导入数据，默认是4个，最好不要将数字设置为高于集群的节点数 –query，-e 从查询结果中导入数据，该参数使用时必须指定–target-dir、–hive-table，在查询语句中一定要有where条件且在where条件中需要包含$CONDITIONS，示例：–query ‘select * from person where $CONDITIONS ‘ –target-dir /user/hive/warehouse/person –hive-table person –split-by |表的列名，用来切分工作单元，一般后面跟主键ID–table |关系数据库表名，数据从该表中获取–target-dir |指定hdfs路径–warehouse-dir |与–target-dir不能同时使用，指定数据导入的存放目录，适用于hdfs导入，不适合导入hive目录–where 从关系数据库导入数据时的查询条件，示例：–where ‘id = 2’-z,–compress |压缩参数，默认情况下数据是没被压缩的，通过该参数可以使用gzip压缩算法对数据进行压缩，适用于SequenceFile, text文本文件, 和Avro文件–compression-codec |Hadoop压缩编码，默认是gzip–null-string |可选参数，如果没有指定，则字符串null将被使用–null-non-string |可选参数，如果没有指定，则字符串null将被使用 从HDFS导入数据库1sqoop export --table WFC_WEATHER -connect "jdbc:mysql://117.37.36.199:3306/hadoopcg?useUnicode=true&amp;characterEncoding=UTF-8" --username root --password root --export-dir /demos/part-r-00000 --input-fields-terminated-by ',' -m 10 参数 说明 –direct 快速模式，利用了数据库的导入工具，如mysql的mysqlimport，可以比jdbc连接的方式更为高效的将数据导入到关系数据库中。 –export-dir 存放数据的HDFS的源目录 -m,–num-mappers 启动N个map来并行导入数据，默认是4个，最好不要将数字设置为高于集群的最大Map数 –table 要导入到的关系数据库表 –update-key 后面接条件列名，通过该参数，可以将关系数据库中已经存在的数据进行更新操作，类似于关系数据库中的update操作 –update-mode 更新模式，有两个值updateonly和默认的allowinsert，该参数只能是在关系数据表里不存在要导入的记录时才能使用，比如要导入的hdfs中有一条id=1的记录，如果在表里已经有一条记录id=2，那么更新会失败。 –input-null-string 可选参数，如果没有指定，则字符串null将被使用 –input-null-non-string 可选参数，如果没有指定，则字符串null将被使用 –staging-table 该参数是用来保证在数据导入关系数据库表的过程中事务安全性的，因为在导入的过程中可能会有多个事务，那么一个事务失败会影响到其它事务，比如导入的数据会出现错误或出现重复的记录等等情况，那么通过该参数可以避免这种情况。创建一个与导入目标表同样的数据结构，保留该表为空在运行数据导入前，所有事务会将结果先存放在该表中，然后最后由该表通过一次事务将结果写入到目标表中。 –clear-staging-table 如果该staging-table非空，则通过该参数可以在运行导入前清除staging-table里的数据。 –batch 该模式用于执行基本语句（暂时还不太清楚含义）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hadoop环境搭建-hadoop2.6.0安装配置，及环境概述]]></title>
      <url>%2F2017%2F02%2F06%2Fhadoop2.6.0%E5%AE%89%E8%A3%85%2F</url>
      <content type="text"><![CDATA[前一阵子弄了1个多月的hadoop环境，基本把环境安装了并且能够进行简单的数据分析，把安装的过程记录下来供参考。 前言因为在windows上进行的研究，全程使用VMware虚拟机，会专门写篇文章将VMware虚拟这里不详细介绍了。 环境准备安装的CentOS6.5系统，用到的软件有名称|版本|地址–|–|–hadoop|2.6.0|http://apache.fayea.com/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gzsqoop|1.4.6|http://mirror.bit.edu.cn/apache/sqoop/1.4.6/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gzflume|1.7.0|http://mirrors.hust.edu.cn/apache/flume/1.7.0/apache-flume-1.7.0-bin.tar.gzhive|2.1.0|http://mirror.bit.edu.cn/apache/hive/stable-2/apache-hive-2.1.0-bin.tar.gzpresto|0.157|https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.157/presto-server-0.157.tar.gzpresto-cli|0.157|https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.157/presto-cli-0.157-executable.jar 环境搭建 ps:推荐最高就用2.6.0的环境了，因为官方给的windows调试版本就是2.6.0，在高的版本没有 winutil.exe 与 hadoop.dll 这2个文件在后续的开发中，无法再windows端进行调试。 伪分布式环境搭建基础配置 hadoop使用JAVA开发，所以需要的最基本环境就是JAVA，安装JDK 创建文件夹 1mkdir /usr/java 解压 1tar -zxvf jdk-1.8.0_111-linux.tar.gz -C /usr/java/ 将java添加到环境变量中 1vim /etc/profile 123#在文件最后添加export JAVA_HOME=/usr/java/jdk1.8.0_111export PATH=$PATH:$JAVA_HOME/bin 刷新配置 1source /etc/profile 下载hadoop 將下载好的hadoop2.6.0上传到/usr/hadoop 解压 1tar -zxvf hadoop-2.6.0.tar.gz 修改 /usr/hadoop/hadoop2.6.0/etc/hadoop下的5个配置文件，分别为core-site.xml|hdfs-site.xml|hadoop-env.sh|mapred-site.xml|yarn-site.xml core-site.xml 增加内容 12345678910111213141516171819 &lt;!-- 制定HDFS（NameNode）的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/hadoop-2.6.0/tmp&lt;/value&gt; &lt;/property&gt;&lt;!-- 配置当前用户，我这里是root为所有用户和组，表示可以使用任意用户向HDFS中增加数据，调试时使用，方便调试--&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; hdfs-site.xml 增加内容 123456789&lt;!-- 指定HDFS副本的数量，伪分布指定为1 --&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;!-- 关闭HDFS的权限控制，使任意用户可以操作，开发时使用，方便本地调试MAPREDUCE--&gt;&lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; hadoop-env.sh 修改内容 1234#JDK安装路径export JAVA_HOME=/usr/java/jdk1.8.0_111export HADOOP_CONF_DIR=$&#123;HADOOP_CONF_DIR:-"/etc/hadoop"&#125; mapred-site.xml 增加内容 12345&lt;!-- 指定mr运行在yarn上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; yarn-site.xml 增加内容 12345678910&lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop&lt;/value&gt;&lt;/property&gt;&lt;!-- reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt; 修改 /etc/profile 配置HADOOP_HOME 12export HADOOP_HOME=/etc/hadoop/hadoop-2.6.0export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 刷新配置 1source /etc/profile 启动命令 第一次启动需要格式化NameNode，以后再次启动时不需要格式化。 1hadoop namenode -format 执行脚本启动hdfs 1start-dfs.sh 执行脚本启动yarn 1start-yarn.sh 验证启动使用jps命令验证12345627408 NameNode28218 Jps27643 SecondaryNameNode28066 NodeManager27803 ResourceManager27512 DataNode 访问地址http://192.168.8.118:50070 （HDFS管理界面）http://192.168.8.118:8088 （MR管理界面） IP地址为机器地址。 如果不能访问可能是因为防火墙未关闭使用命令关闭防火墙12345678#查看防火墙状态service iptables status#关闭防火墙service iptables stop#查看防火墙开机启动状态chkconfig iptables --list#关闭防火墙开机启动chkconfig iptables off 配置SSH免密码登录进入到我的home目录123456cd ~/.sshssh-keygen -t rsa （四个回车）执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）将公钥拷贝到要免登陆的机器上ssh-copy-id localhostscp -r authorized_keys root@192.168.109.130:/root/.ssh/ 之后每次启动都不需输入密码了]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ShadowSocks（影梭）]]></title>
      <url>%2F2017%2F02%2F06%2FShadowSocks%EF%BC%88%E5%BD%B1%E6%A2%AD%EF%BC%89%2F</url>
      <content type="text"><![CDATA[影梭也用了一段时间，最近刚好把使用中遇到的问题分享一下。里面好些内容都是网上摘抄的，感觉比我自己讲的好很多，看了很受启发也就拿来用了。 什么是 ShadowSocks （影梭）ShadowSocks 是由@clowwindy所开发的一个开源 Socks5 代理。如其官网所言 ，它是 “A secure socks5 proxy, designed to protect your Internet traffic” （一个安全的 Socks5 代理）。其作用，亦如该项目主页的 wiki（中文版） 中所说，“A fast tunnel proxy that helps you bypass firewalls” （一个可穿透防火墙的快速代理）。 不过，在中国，由于GFW的存在，更多的网友用它来进行科学上网。 ShadowSocks 的原理这里推荐“写给非专业人士看的 Shadowsocks 简介”，讲的非常清楚。为防止有同学无法访问该文章，这里摘抄出来： long long ago… 在很久很久以前，我们访问各种网站都是简单而直接的，用户的请求通过互联网发送到服务提供方，服务提供方直接将信息反馈给用户 when evil comes 然后有一天，GFW 就出现了，他像一个收过路费的强盗一样夹在了在用户和服务之间，每当用户需要获取信息，都经过了 GFW，GFW将它不喜欢的内容统统过滤掉，于是客户当触发 GFW 的过滤规则的时候，就会收到 Connection Reset 这样的响应内容，而无法接收到正常的内容 ssh tunnel 聪明的人们想到了利用境外服务器代理的方法来绕过 GFW 的过滤，其中包含了各种HTTP代理服务、Socks服务、VPN服务… 其中以 ssh tunnel 的方法比较有代表性 1) 首先用户和境外服务器基于 ssh 建立起一条加密的通道2-3) 用户通过建立起的隧道进行代理，通过 ssh server 向真实的服务发起请求4-5) 服务通过 ssh server，再通过创建好的隧道返回给用户 由于 ssh 本身就是基于 RSA 加密技术，所以 GFW 无法从数据传输的过程中的加密数据内容进行关键词分析，避免了被重置链接的问题，但由于创建隧道和数据传输的过程中，ssh 本身的特征是明显的，所以 GFW 一度通过分析连接的特征进行干扰，导致 ssh存在被定向进行干扰的问题 shadowsocks 于是 clowwindy 同学分享并开源了他的解决方案 简单理解的话，shadowsocks 是将原来 ssh 创建的 Socks5 协议拆开成 server 端和 client 端，所以下面这个原理图基本上和利用 ssh tunnel 大致类似 1、6) 客户端发出的请求基于 Socks5 协议跟 ss-local 端进行通讯，由于这个 ss-local 一般是本机或路由器或局域网的其他机器，不经过 GFW，所以解决了上面被 GFW 通过特征分析进行干扰的问题2、5) ss-local 和 ss-server 两端通过多种可选的加密方法进行通讯，经过 GFW 的时候是常规的TCP包，没有明显的特征码而且 GFW 也无法对通讯数据进行解密3、4) ss-server 将收到的加密数据进行解密，还原原来的请求，再发送到用户需要访问的服务，获取响应原路返回 Shadowsocks 的优劣优势 支持远程 DNS 解析，防止 DNS 污染。由于 socks5 代理支持远程 DNS 解析，因此不用另外再去找国外的 DNS 服务器，DNS查询直接递给远程代理服务器，然后通过墙外 DNS 服务器查询得到结果再传回客户端。从而 DNS 污染鞭长莫及。 安全。所有数据流量全部经过加密，加密算法可选并支持自定义算法。另外，远程 DNS 解析也使得本地的 ISP 无法通过 DNS 查询获取你所访问的网站。 隐蔽。 OPENVPN 和 VPNgate 都是死在了特征检测上，通常来说基于证书的身份认证过程和密钥交换过程都会带来独特的协议指纹（ OPENVPN 有着一套复杂完善的身份认证机制，估计 GFW 就是识别出了这一机制的协议指纹从而成功干掉 OPENVPN 的），从而使得他们在 handshake 阶段就被 GFW 识别出来并阻断了；但 ShadowSocks 直接放弃了服务器端身份认证，也抛弃了密钥协商过程（ TLS 连接就是在 handshake 阶段协商出随机密钥的），而是采取事先在服务器端设置好固定密钥的方式来应对加密连接的（设置shadowsocks客户端和服务器端的时候要填写同一个密码，这就是事先设置好的用于加密和解密的密钥）。这样做就大大减少了协议特征，再加上一般的 ShadowSocks 服务器端都是个人租用的专用服务器，流量很小，从而很难被 GFW 发现和封杀。 速度相对较快。由于其隐蔽性，只会有很少的数据包会被 GFW 丢弃，从而保证了连接速度。 连接稳定。同样由于其隐蔽性，较小的丢包率带来的是稳定的连接。智能切换。 传统的 VPN 方式，在切换网络时非常不方便， 比如连上国外的 VPN 之后会发现访问国内的网站速度严重下降。 ShadowSocks 支持 PAC 列表，根据 PAC 中的规则，有针对性地选用恰当的网络访问方式，兼顾了访问速度与访问效率。移动客户端还支持针对不同应用设置单独代理。 去中心化。服务器端搭建方便快捷，每个人都可以自己动手搭建属于自己的服务器端。部分人以免费或者收费方式共享自己的服务器，即使不想动手搭建的也有很多的免费账号或购买渠道可以选择。代码开源。不像某些蜜罐式的翻墙工具，开源的代码保证了无后门，从而为上网的隐私性与安全性提供保障。客户端配置简单。配置时只需要填写 IP /域名、端口号，密码，然后选择加密方式即可。客户端绿色小巧。Windows版本的客户端只有200多k，而且免安装，解压即可使用。 省电。在移动端上使用，电量管理中几乎看不到它的身影。支持开机启动，断网无影响，无需手动重连，方便网络不稳定或者3G&amp;Wi-Fi频繁切换的小伙伴。跨平台。支持主流系统包括 Windows， Linux， Mac， Android, IOS，都有对应的客户端支持。 劣势若自己搭建 ShadowSocks 服务，需要一定的 技术成本。因为大部分服务端是基于运行 Linux 的 VPS（虚拟主机）搭建，因此需要学会使用 putty 等远程管理工具的使用方法，并掌握一定的Linux基本命令行操作。金钱成本。租用国外的服务器，需要价格不菲的费用。若使用免费 ShadowSocks 账号，可能有一定的风险。 时间精力成本。很多免费账号由于使用者众多，人均流量很小，导致网速慢，不稳定，需要不断寻找新的替代，且很多免费账号会定时更新密码，或者是有流量限制，或者是需要定时签到等等，这些都需要花费时间和精力去一一满足需求才能短暂使用。安全成本。有一些人会将自己的 ShadowSocks 节点免费分享出来，这些人是非常值得肯定和称赞的！但是其中有一小部分人别有居心，例如在 ShadowSocks 的服务器端监听网络流量，进行中间人攻击等，这类钓鱼服务器会严重损害用户的个人信息安全。若购买收费的 ShadowSocks 账号， 需要一定的金钱。 金钱成本 由于很多 ShadowSocks 卖家都是以盈利为目的，需要花费一定的费用。那么，自己想要有一个影梭账号的话，到底是选用哪种比较好呢？可以看到网上很多卖 ShadowSocks 账号的，有的价格很便宜，比自己搭服务器划算多了，是不是直接购买一个账号比较好呢？在这里我建议大家自己搭服务器，或者找几个人一起租个服务器比较划算。不推荐购买商业出售的 ShadowSocks。商业的出售 ShadowSocks 账号的行为绝大部分都是耍流氓。试着算一下，作为一个 ShadowSocks 出售者，他们的成本是服务器的租用费用，而收入是购买 ShadowSocks 账号人数 × 每人花费的 ShadowSocks 账号购买价格。我们假设，一台服务器的租用成本，是 10 美元一个月，那么如果要想不亏本，且做到低价，假设现在卖给十个人，那么至少每个人每月要 1 美元。但是，服务器的带宽是有限的，假设是 100M 的带宽，那么平均分下来人均带宽只有 10M，而服务器的流量一般也是有限的，如果一共是 1000 M 的流量，那么每人每月只分到 100M 。如果超出流量，服务器的租用费用会增加，就好像我们手机流量超出后，额外的流量需要交钱一样。因此，购买的流量一般都会受限制。从上面的例子可以看到，服务器资源是大家共享的，使用的人越多，人均分到的资源就越少。但是， ShadowSocks 的卖家需要赚钱，那么怎么办呢？当然是最小化成本，并最大化收入了。也就是说，尽量租用少量廉价的服务器，然后将它以尽可能高的价格卖给尽可能多的消费者。因此，对于 ShadowSocks 的商家而言，超售（即一台服务器原本最多10个人用的，可能最后卖了100个人）的现象非常严重，消费者最终所能享受到的流量和连接速度显然对不起自己的花费。不如自己租个服务器，不仅能有一个稳定放心的服务器，还能顺便学习些 Linux 的相关知识。如果嫌麻烦，可以找那种大家一起租一个服务器的，相对而言，至少有一点可以保证，就是不会有各种奇奇怪怪的限制，比如不能发邮件，不能看视频，不能下载特定的某些资源等等。而且最大的一个好处是，租用的人数一般较少，最重要的是这种一般不用担心超售，各方面性能配置等有保证。ShadowSocks 配置 服务器端配置服务器的选择首先是选择一款合适的国外主机作为服务器，一般而言，用来作 ShadowSocks 的服务器的话，并不需要一台独立的国外主机，只需要选择一款虚拟主机（Virtual Private Server，简称 VPS）即可满足需要。这里推荐几家较常用的VPS。 BandwagonHOST（俗称搬瓦工）,Vultr,Linode,枫叶说实话除了搬瓦工都不推荐，其他的太贵了，而且搬瓦工有一键部署SS服务端的功能，也支持支付宝支付，对于只作为翻墙用的服务器是首选。 服务端安装安装 python 的 pip1yum install python-setuptools &amp;&amp; easy_install pip 然后安装 shadowsocks1pip install shadowsocks 增加配置文件1vi /etc/shadowsocks.json 内容如下12345678910&#123; "server":"your_server_ip", "server_port":8388, "local_address": "127.0.0.1", "local_port":1080, "password":"auooo.com", "timeout":300, "method":"aes-256-cfb", "fast_open": false&#125; 名称 说明 server 填入你的服务器 IP ，即当前操作的 VPS 的 IP 地址，必须修改 server_port 服务器端口，可以根据实际需要修改，或者保持默认 local_address 本地监听地址，建议保持默认 local_port 本地端口，这个参数一般保持默认即可 password 用来加密的密码，可以根据实际需要修改 timeout 单位秒，一般保持默认即可 method 默认的是”aes-256-cfb”，一般保持默认即可 fast_open 使用TCP_FASTOPEN, 参数选项true / false，一般保持默认即可 workers worker的数量, 在 Unix/Linux 上有效，一般不用加此项 启动1ssserver -c /etc/shadowsocks.json -d start 停止1ssserver -c /etc/shadowsocks.json -d stop 增加开机启动1vi /etc/rc.local 增加1ssserver -c /etc/shadowsocks.json -d start 搬瓦工一键安装搬瓦工开发了一键安装SS服务的功能，在控制台中选择安装就OK。[server 安装](http://shadowsocks.org/en/download/servers.html)[配置文件](http://shadowsocks.org/en/config/quick-guide.html) 客户端客户端下载下载地址提取码:z6d6 客户端配置配置在服务端JSON中设置的值就可以了 作者回答问题 当时几个版本内置的公共服务器流量爆了，打算把公共服务器给 @showfom 运营，并帮 @showfom 架设了服务。但没想到实际情况变成了用名气来进行宣传，也没想到这个服务会被用户当成“官方”，所以为了避免这种误解，加上精力也不够，还是取消了合作，没有收一分钱。在 Twitter 发过澄清，没在 V2EX 发是一个疏忽。至于在代码里加红字警告? Are you kidding? 一个开源项目，代码本身应该是中立的，是不应该有立场的。 为了避免今后出现类似的情况，在这里做一个声明：Shadowsocks 是一个由很多人参与的开源项目，它只是一个程序，一套算法。这些人是以自由的形式凭自己的兴趣参与这个项目的，不存在“Shadowsocks 团队”，只存在贡献者。哪天现在的这些人不维护这些代码了，也会有其他人继续维护下去。至于 Shadowsocks 的使用，只要不违反开源授权，不会干涉。 公共服务器还是免费，只是做了限速以免流量用超。OS X 版的公共服务器速度慢到不能用的时候直接关掉了。和 GFW 捉迷藏换 IP 的游戏，虽然已经半自动化，但一点也不好玩，你们明白 fqrouter 为什么要关了吧。 从实际情况上来看，Shadowsocks 没有办法离开去中心化的服务器。要么自己花钱买 VPS，要么用有人分享的账号，要么用有人提供的付费服务，他们各有所长，适合不同的人。所以作为开发者，保持中立，不偏袒其中任何一方，顺其自然发展下去是最好的吧。 很多人要么一窝蜂的支持，要么一窝蜂的反对，还要把它给封禁掉，大概这种心理鲁迅先生也曾批判过。我还记得当年极路由在 V2EX 被冤枉的时候，也是一边倒的说极路由窃取隐私。如果你们真的那么讨厌商业，那你们应该首先把你们的苹果设备给摔了，因为它就是商业社会巅峰造极的产物。我反对不喜欢一个东西就要拿出简单粗暴的制裁手段，正是这种习性成就了 GFW。 维护这个项目到现在大概总共回复过几千个问题，开始慢慢想清楚了一件事，为什么会存在 GFW。从这些提问可以看出，大部分人的自理能力都很差，只是等着别人帮他。特别是那些从 App Store 下载了 App 用着公共服务器的人，经常发来一封只有四个字的邮件：“不能用了？” 我觉得这是一个社会常识，花一分钟写的问题，不能期待一个毫无交情的陌生人花一个小时耐心地问你版本和操作步骤，模拟出你的环境来帮你分析解决。Windows 版加上 GFWList 功能以来，我反复呼吁给 GFWList 提交规则，但是一个月过去了竟然一个提交都没有。如果没有人做一点什么，它自己是不会更新的啊，没有人会义务地帮你打理这些。最近 net-speeder 又开始流行，害人害己。我觉得，政府无限的权力，都是大部分人自己放弃的。假货坑爹，让政府审核。孩子管不好，让政府关网吧。有人在微博骂我，让政府去删。房价太高，让政府去限购。我们的文化实在太独特，创造出了家长式威权政府，GFW 正是在这种背景下产生的，一个社会矛盾的终极调和器，最终生活不能自理的你每天做的每一件事情都要给政府审查一遍，以免伤害到其他同样生活不能自理的人。这是一个零和游戏，越和这样的用户打交道，越对未来持悲观态度，觉得 GFW 可能永远也不会消失，而墙内的这个局域网看起来还似乎生机勃勃的自成一体，真是让人绝望。顺便回答几个其它的问题吧。 为什么项目的文档和网站是英文的？开源项目用英文来维护本来就是一个约定俗成的做法。网络审查和屏蔽不是一个国家的事情，而是一个世界性的问题。这些项目有很多其它国家的用户，也有一些国外的大牛作出过宝贵的贡献。 参与的人只是凭兴趣做自己喜欢的事情，如果你的 issue 不像是一种思考和贡献，而更像是咨询客服，那被关掉了不要觉得惊讶。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hexo+githubpage安装配置]]></title>
      <url>%2F2017%2F02%2F03%2Fhexo%2Bgithub%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
      <content type="text"><![CDATA[最近终于用hexo+githubpage把个人博客建立了，中间遇到不少的坑，记下来省的再犯。 githubpagegithubpage真的是个好项目，为我们广大屌丝解决了很多问题具体的不想详细介绍不管是度娘还是谷哥都是一大堆。就给个地址: githubpage hexohexo也没啥说的官网巨详细基本就是照着做就可以地址:hexo themeshexo的社区很活跃提供的样式也很多这里选择了 next 这个被加星最多的样式，感觉还是很不错的该有的功能一应俱全 nextnext 主题功能全面，这里我只把个人修改的地方做个介绍 我修改了主题里的footer模板，原因是我对HEXO强力驱动和next主题支持这个标签实在是无爱了footer模板位置：themes\next\layout_partials\footer.swig模板很容易看懂，修改自己需要的内容就可以了。ps:注意里面变量的值是从你设置的对应语言的YML中获取的，我设置的简体中文。文件位置在themes/next/languages/zh-Hans.yml next的标签页面我不太喜欢，或者说是hexo的tagcloud样子我觉得都不是太爱。所以修改了样式，参考了freemind的标签样式 修改模板 themes\next\page.swig 替换了原有的标签生成修改为自定的 div与样式 12345678910111213141516&lt;div id="posts" class="posts-expand"&gt;&#123;% include '_partials/page-header.swig' %&#125; &#123;# tagcloud page support #&#125; &#123;% if page.type === "tags" %&#125; &lt;div class="tag-cloud"&gt; &lt;div class="tag-cloud-title"&gt; &#123;&#123; _p('counter.tag_cloud', site.tags.length) &#125;&#125; &lt;/div&gt; &lt;div class="widget"&gt; &lt;ul class="tag_box inline list-unstyled"&gt; &#123;% for item in site.tags %&#125; &lt;li&gt;&lt;a href="&#123;&#123;config.root&#125;&#125;&#123;&#123;item.path&#125;&#125;"&gt;&#123;&#123;item.name&#125;&#125;&lt;span&gt;&#123;&#123;item.posts.length&#125;&#125;&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; 修改样式 themes\next\source\css_common\components\post\post-tags.styl中增加 123456789101112131415161718192021222324252627282930313233343536373839.widget &#123; padding-bottom: 25px; border-bottom: 1px solid #e0e0e0;&#125;.tag_box &#123; margin:0; overflow:hidden;&#125;.tag_box li &#123; line-height:28px;&#125;.tag_box li i &#123; opacity:0.9;&#125;.tag_box.inline li &#123; float:left;&#125;.tag_box a &#123; padding: 2px 6px; margin: 2px; background: #e5e5e5; color:#555; border-radius: 3px; text-decoration:none; border:1px dashed #bbb;&#125;.tag_box a span&#123; vertical-align:super; font-size:0.8em;&#125;.tag_box a:hover &#123; background-color:#397bdd; color:#FFF;&#125;.tag_box a.active &#123; background:#57A957; border:1px solid #4C964D; color:#FFF;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[github CNAME 设置]]></title>
      <url>%2F2017%2F02%2F03%2FCNAME%E9%85%8D%E7%BD%AE%2F</url>
      <content type="text"><![CDATA[每次提交页面到githubpage上后总是受到一封警告邮件，虽说不影响访问但是被骚扰的很烦，(github的好意实在是太频繁了) The page build completed successfully, but returned the following warning:Your site’s DNS settings are using a custom subdomain, www.wenfanchao.win, that’s set up as an A record. We recommend you change this to a CNAME record pointing at wenfanchao.github.io. For more information, see https://help.github.com/pages/.For information on troubleshooting Jekyll see:https://help.github.com/articles/troubleshooting-jekyll-builds 有用的核心就是高亮的那句，意思很明确你的域名没有指向一个正确的地址 解决办法 首先在hexo的suorce中增加CNAME文件，内容就是你购买的域名 1www.wenfanchao.win 然后就在万网控制台修改A记录地址并增加CNAMECNAME的值就是你githubpage地址:wenfanchao.github.io 使用dig命令获取A记录的值1dig wenfanchao.github.io +nostats +nocomments +nocmd 注意wenfanchao.github.io为你博客的地址。 windows安装dig到百度云盘下载如下dig工具地址：链接：http://pan.baidu.com/s/1gedd9WB 密码：wtr3安装完毕后在环境变量中添加就能使用了]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hexo模板之ejs与Swig的优劣]]></title>
      <url>%2F2017%2F02%2F02%2Fhexo%E6%A8%A1%E6%9D%BF%E4%B9%8Bejs%E4%B8%8ESwig%E7%9A%84%E4%BC%98%E5%8A%A3%2F</url>
      <content type="text"><![CDATA[2个模板hexo刚入坑，随便从网上down了2个themes一个next，一个freemind 简单说一下认识ejs比swig语法更丰富当然这和2个模板的定位不一样,但是对我这种懒人来说EJS更好用为啥就举例一个 ejs1site.tags.random().limit(20) swig没有直接导致我要随机输出标签的功能完蛋。 swig只能老实的123&#123;% for item in site.tags %&#125; &lt;li&gt;&lt;a href=&quot;&#123;&#123;config.root&#125;&#125;&#123;&#123;item.path&#125;&#125;&quot;&gt;&#123;&#123;item.name&#125;&#125;&lt;span&gt;&#123;&#123;item.posts.length&#125;&#125;&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&#123;% endfor %&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2016%2F12%2F03%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new [layout] &lt;title&gt; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
    </entry>

    
  
  
</search>
